% Template for PLoS
% Version 3.5 March 2018
%
% % % % % % % % % % % % % % % % % % % % % %
%
% -- IMPORTANT NOTE
%
% This template contains comments intended
% to minimize problems and delays during our production
% process. Please follow the template instructions
% whenever possible.
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% Once your paper is accepted for publication,
% PLEASE REMOVE ALL TRACKED CHANGES in this file
% and leave only the final text of your manuscript.
% PLOS recommends the use of latexdiff to track changes during review, as this will help to maintain a clean tex file.
% Visit https://www.ctan.org/pkg/latexdiff?lang=en for info or contact us at latex@plos.org.
%
%
% There are no restrictions on package use within the LaTeX files except that
% no packages listed in the template may be deleted.
%
% Please do not include colors or graphics in the text.
%
% The manuscript LaTeX source should be contained within a single file (do not use \input, \externaldocument, or similar commands).
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% -- FIGURES AND TABLES
%
% Please include tables/figure captions directly after the paragraph where they are first cited in the text.
%
% DO NOT INCLUDE GRAPHICS IN YOUR MANUSCRIPT
% - Figures should be uploaded separately from your manuscript file.
% - Figures generated using LaTeX should be extracted and removed from the PDF before submission.
% - Figures containing multiple panels/subfigures must be combined into one image file before submission.
% For figure citations, please use "Fig" instead of "Figure".
% See http://journals.plos.org/plosone/s/figures for PLOS figure guidelines.
%
% Tables should be cell-based and may not contain:
% - spacing/line breaks within cells to alter layout or alignment
% - do not nest tabular environments (no tabular environments within tabular environments)
% - no graphics or colored text (cell background color/shading OK)
% See http://journals.plos.org/plosone/s/tables for table guidelines.
%
% For tables that exceed the width of the text column, use the adjustwidth environment as illustrated in the example table in text below.
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% -- EQUATIONS, MATH SYMBOLS, SUBSCRIPTS, AND SUPERSCRIPTS
%
% IMPORTANT
% Below are a few tips to help format your equations and other special characters according to our specifications. For more tips to help reduce the possibility of formatting errors during conversion, please see our LaTeX guidelines at http://journals.plos.org/plosone/s/latex
%
% For inline equations, please be sure to include all portions of an equation in the math environment.
%
% Do not include text that is not math in the math environment.
%
% Please add line breaks to long display equations when possible in order to fit size of the column.
%
% For inline equations, please do not include punctuation (commas, etc) within the math environment unless this is part of the equation.
%
% When adding superscript or subscripts outside of brackets/braces, please group using {}.
%
% Do not use \cal for caligraphic font.  Instead, use \mathcal{}
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% Please contact latex@plos.org with any questions.
%
% % % % % % % % % % % % % % % % % % % % % % % %

\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% Use Unicode characters when possible
\usepackage[utf8x]{inputenc}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
% \usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
%\usepackage{setspace}
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
% \bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother



% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
%\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
%\setlength{\headheight}{27.023pt}
%\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\today}

%% Include all macros below

\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}


% tightlist command for lists without linebreak
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



\usepackage{amsmath,amsfonts,amssymb}
\usepackage{bm, bbm}


\usepackage{forarray}
\usepackage{xstring}
\newcommand{\getIndex}[2]{
  \ForEach{,}{\IfEq{#1}{\thislevelitem}{\number\thislevelcount\ExitForEach}{}}{#2}
}

\setcounter{secnumdepth}{0}

\newcommand{\getAff}[1]{
  \getIndex{#1}{USEPA,STLAWU,NOAA}
}

\begin{document}
\vspace*{0.2in}


% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{spmodel: spatial statistical modeling and prediction in
\textbf{\textsf{R}}} % Please use "sentence case" for title and headings (capitalize only the first word in a title (or heading), the first word in a subtitle (or subheading), and any proper nouns).
}
\newline
% Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
\\
Michael Dumelle\textsuperscript{\getAff{USEPA}}\textsuperscript{*},
Matt Higham\textsuperscript{\getAff{STLAWU}},
Jay M. Ver Hoef\textsuperscript{\getAff{NOAA}}\\
\bigskip
\textbf{\getAff{USEPA}}United States Environmental Protection Agency, Corvallis, Oregon, United
States of America\\
\textbf{\getAff{STLAWU}}St.~Lawrence University Department of Math, Computer Science, and
Statistics, Canton, New York, United States of America\\
\textbf{\getAff{NOAA}}National Oceanic and Atmospheric Administration Alaska Fisheries Science
Center, Marine Mammal Laboratory, Seattle, Washington, United States of
America\\
\bigskip
* Corresponding author: Dumelle.Michael@epa.gov\\
\end{flushleft}
% Please keep the abstract below 300 words
\section*{Abstract}
\texttt{spmodel} is an \textbf{\textsf{R}} package used to fit,
summarize, and predict for a variety spatial statistical models applied
to point-referenced or areal (lattice) data. Parameters are estimated
using various methods. Additional modeling features include anisotropy,
random effects, partition factors, big data approaches, and more.
Model-fit statistics are used to summarize, visualize, and compare
models. Predictions at unobserved locations are readily obtainable.

% Please keep the Author Summary between 150 and 200 words
% Use first person. PLOS ONE authors please skip this step.
% Author Summary not valid for PLOS ONE submissions.

\linenumbers

% Use "Eq" instead of "Equation" for equation citations.
\hypertarget{sec:introduction}{%
\section{Introduction}\label{sec:introduction}}

Spatial data are ubiquitous in everyday life and the scientific
literature. As such, it is becoming increasingly important to properly
analyze spatial data. Spatial data can be analyzed using a statistical
model that explicitly incorporates the spatial dependence among nearby
observations. Incorporating this spatial dependence can be challenging,
but ignoring it often yields poor statistical models that incorrectly
quantify uncertainty, impacting the validity of hypothesis tests,
confidence intervals, and predictions intervals. \texttt{spmodel}
provides tools to easily incorporate spatial dependence into statistical
models, building upon commonly used \(\textbf{\textsf{R}}\) functions
like \texttt{lm()}.

\texttt{spmodel} implements model-based inference, which relies on
fitting a statistical model. Model-based inference is different than
design-based inference, which relies on random sampling and estimators
that incorporate the properties of the random sample {[}1{]}. {[}2{]}
defines two types of spatial data that can be analyzed using model-based
inference: point-referenced data and areal data (areal data are
sometimes called lattice data). Spatial data are point-referenced when
they are observed at point-locations indexed by x-coordinates and
y-coordinates on a spatially continuous surface with an infinite number
of locations. Spatial models for point-referenced data are sometimes
called geostatistical models. Spatial data are areal when they are
observed as part of a finite network of polygons whose connections are
indexed by a neighborhood structure. For example, the polygons may
represent counties in a state who are neighbors if they share at least
one boundary. Spatial models for areal data are sometimes called spatial
autoregressive models. For thorough overviews model-based inference in a
spatial context, see {[}2{]}, {[}3{]}, and {[}4{]}.

Several \(\textbf{\textsf{R}}\) packages exist on CRAN that analyze
either point-referenced or areal spatial data. For point-referenced
data, they include \texttt{fields} {[}5{]}, \texttt{FRK} {[}6{]},
\texttt{geoR} {[}7{]}, \texttt{GpGp} {[}8{]}, \texttt{gstat} {[}9{]},
\texttt{LatticeKrig} {[}10{]}, \texttt{R-inla} {[}11{]}, \texttt{rstan}
{[}12{]}, \texttt{spatial} {[}13{]}, \texttt{spBayes} {[}14{]}, and
\texttt{spNNGP} {[}15{]}. For areal data, they include \texttt{brms}
{[}16{]}, \texttt{CARBayes} {[}17{]}, bigDM {[}18{]}, and \texttt{hglm}
{[}19{]}. Unlike these aforementioned packages, \texttt{spmodel} is
designed to analyze both point-referenced and areal data using a common
framework and syntax structure. \texttt{spmodel} also offers many
features missing from the aforementioned \(\textbf{\textsf{R}}\)
packages -- together in one \(\textbf{\textsf{R}}\) package,
\texttt{spmodel} offers detailed model summaries, extensive model
diagnostics, non-spatial random effects, anisotropy, big data methods,
prediction, the option to fix spatial covariance parameters at known
values, and more.

The rest of this article is organized as follows. We first give a brief
theoretical introduction to spatial linear models. We then outline the
variety of methods used to estimate the parameters of spatial linear
models. Next we explain how to obtain predictions at unobserved
locations. Following that, we detail some advanced modeling features,
including random effects, partition factors, anisotropy, and big data
approaches. Finally we end with a short discussion.

Before proceeding, we install \texttt{spmodel} from CRAN and load it by
running

\begin{verbatim}
R> install.packages("spmodel")
R> library(spmodel)
\end{verbatim}

We create visualizations using ggplot2 {[}20{]}, which we install from
CRAN and load by running

\begin{verbatim}
R> install.packages("ggplot2")
R> library(ggplot2)
\end{verbatim}

We also show code that can be used to create interactive visualizations
of spatial data with \texttt{mapview} {[}21{]}. \texttt{mapview} has
many backgrounds available that contextualize spatial data with
topographical information. Before running the \texttt{mapview} code
provided interactively, make sure that \texttt{mapview} is installed and
loaded.

\texttt{spmodel} contains various methods for generic functions defined
outside of \texttt{spmodel}. To find relevant documentation for these
methods, run \texttt{help("generic.spmodel",\ "spmodel")} (e.g.,
\texttt{help("fitted.spmodel",\ "spmodel")},
\texttt{help("summary.spmodel",\ "spmodel")},
\texttt{help("plot.spmodel",\ "spmodel")},
\texttt{help("predict.spmodel",\ "spmodel")},
\texttt{help("tidy.spmodel",\ "spmodel")}, etc.) . We provide more
details and examples regarding these methods and generics throughout
this vignette. For a full list of \texttt{spmodel} functions available,
see \texttt{spmodel}'s documentation manual.

\hypertarget{sec:theomodel}{%
\section{The Spatial Linear Model}\label{sec:theomodel}}

Statistical linear models are often parameterized as
\begin{equation}\label{eq:lm}
 \mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon},
\end{equation} where for a sample size \(n\), \(\mathbf{y}\) is an
\(n \times 1\) column vector of response variables, \(\mathbf{X}\) is an
\(n \times p\) design (model) matrix of explanatory variables,
\(\boldsymbol{\beta}\) is an \(p \times 1\) column vector of fixed
effects controlling the impact of \(\mathbf{X}\) on \(\mathbf{y}\), and
\(\boldsymbol{\epsilon}\) is an \(n \times 1\) column vector of random
errors. We typically assume that
\(\text{E}(\boldsymbol{\epsilon}) = \mathbf{0}\) and
\(\text{Cov}(\boldsymbol{\epsilon}) = \sigma^2_\epsilon \mathbf{I}\),
where \(\text{E}(\cdot)\) denotes expectation, \(\text{Cov}(\cdot)\)
denotes covariance, \(\sigma^2_\epsilon\) denotes a variance parameter,
and \(\mathbf{I}\) denotes the identity matrix.

The model in Equation\(~\)\ref{eq:lm} assumes the elements of
\(\mathbf{y}\) are uncorrelated. Typically for spatial data, elements of
\(\mathbf{y}\) are correlated, as observations close together in space
tend to be more similar than observations far apart {[}22{]}. Failing to
properly accommodate the spatial dependence in \(\mathbf{y}\) can lead
researchers to incorrect conclusions about their data. To accommodate
spatial dependence in \(\mathbf{y}\), an \(n \times 1\) spatial random
effect, \(\boldsymbol{\tau}\), is added to Equation\(~\)\ref{eq:lm},
yielding the model \begin{equation}\label{eq:splm}
 \mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\tau} + \boldsymbol{\epsilon},
\end{equation} where \(\boldsymbol{\tau}\) is independent of
\(\boldsymbol{\epsilon}\), \(\text{E}(\boldsymbol{\tau}) = \mathbf{0}\),
\(\text{Cov}(\boldsymbol{\tau}) = \sigma^2_\tau \mathbf{R}\), and
\(\mathbf{R}\) is a matrix that determines the spatial dependence
structure in \(\mathbf{y}\) and depends on a range parameter, \(\phi\).
We discuss \(\mathbf{R}\) in more detail shortly. The parameter
\(\sigma^2_\tau\) is called the spatially dependent random error
variance or partial sill. The parameter \(\sigma^2_\epsilon\) is called
the spatially independent random error variance or nugget. These two
variance parameters are henceforth more intuitively written as
\(\sigma^2_{de}\) and \(\sigma^2_{ie}\), respectively. The covariance of
\(\mathbf{y}\) is denoted \(\boldsymbol{\Sigma}\) and given by
\(\sigma^2_{de} \mathbf{R} + \sigma^2_{ie} \mathbf{I}\). The parameters
that compose this covariance are typically indexed by the vector
\(\boldsymbol{\theta}\), which is called the covariance parameter
vector.

Equation\(~\)\ref{eq:splm} is called the spatial linear model. The
spatial linear model applies to both point-referenced and areal data.
The \texttt{splm()} function is used to fit spatial linear models for
point-referenced data (i.e., geostatistical models). One spatial
covariance function available in \texttt{splm()} is the exponential
spatial covariance function, which has an \(\mathbf{R}\) matrix given by
\begin{equation}\label{eq:r_exp}
  \mathbf{R} = \exp(-\mathbf{M} / \phi),
\end{equation}\\
where \(\mathbf{M}\) is a matrix of Euclidean distances among
observations. Recall that \(\phi\) is the range parameter, and it
controls the behavior of \(\mathbf{R}\) as a function of distance. In
Equation\(~\)\ref{eq:r_exp}, as the distance between two observations
increases, the correlation between them decreases. Parameterizations for
other \texttt{splm()} spatial covariance types and their \(\mathbf{R}\)
matrices can be viewed by running \texttt{help("splm",\ "spmodel")} or
\texttt{vignette("technical",\ "spmodel")}. Some of these spatial
covariance types (e.g., Mat\(\acute{e}\)rn) depend on an extra parameter
beyond \(\sigma^2_{de}\), \(\sigma^2_{ie}\), and \(\phi\).

The \texttt{spautor()} function is used to fit spatial linear models for
areal data (i.e., spatial autoregressive models). One spatial
autoregressive covariance function available in \texttt{spautor()} is
the simultaneous autoregressive spatial covariance function, which has
an \(\mathbf{R}\) matrix given by \begin{equation*}
  \mathbf{R} = [(\mathbf{I} - \phi \mathbf{W})(\mathbf{I} - \phi \mathbf{W})^\top]^{-1},
\end{equation*} where \(\mathbf{W}\) is a weight matrix describing the
neighborhood structure in \(\mathbf{y}\). Parameterizations for
\texttt{spautor()} spatial covariance types and their \(\mathbf{R}\)
matrices can be seen by running \texttt{help("spautor",\ "spmodel")} or
\texttt{vignette("technical",\ "spmodel")}.

One way to define \(\mathbf{W}\) is through queen contiguity {[}23{]}.
Two observations are queen contiguous if they share a boundary. The
\(ij\)th element of \(\mathbf{W}\) is then one if observation \(i\) and
observation \(j\) are queen contiguous and zero otherwise. Observations
are not considered neighbors with themselves, so each diagonal element
of \(\mathbf{W}\) is zero.

Sometimes each element in the weight matrix \(\mathbf{W}\) is divided by
its respective row sum. This is called row-standardization.
Row-standardizing \(\mathbf{W}\) has several benefits, which are
discussed in detail by {[}24{]}.

\hypertarget{sec:modelfit}{%
\section{Model Fitting}\label{sec:modelfit}}

In this section, we show how to use the \texttt{splm()} and
\texttt{spautor()} functions to estimate parameters of the spatial
linear model. We also explore diagnostic tools in \texttt{spmodel} that
evaluate model fit. The \texttt{splm()} and \texttt{spautor()} functions
share similar syntactic structure with the \texttt{lm()} function used
to fit non-spatial linear models from Equation\(~\)\ref{eq:lm}.
\texttt{splm()} and \texttt{spautor()} generally require at least three
arguments:

\begin{itemize}
\tightlist
\item
  \texttt{formula}: a formula that describes the relationship between
  the response variable (\(\mathbf{y}\)) and explanatory variables
  (\(\mathbf{X}\))

  \begin{itemize}
  \tightlist
  \item
    \texttt{formula} in \texttt{splm()} is the same as \texttt{formula}
    in \texttt{lm()}
  \end{itemize}
\item
  \texttt{data}: a \texttt{data.frame} or \texttt{sf} object that
  contains the response variable, explanatory variables, and spatial
  information
\item
  \texttt{spcov\_type}: the spatial covariance type
  (\texttt{"exponential"}, \texttt{"matern"}, \texttt{"car"}, etc)
\end{itemize}

If \texttt{data} is an \texttt{sf} {[}25{]} object, spatial information
is stored in the object's geometry. If \texttt{data} is a
\texttt{data.frame}, then the x-coordinates and y-coordinates must be
provided via the \texttt{xcoord} and \texttt{ycoord} arguments (for
point-referenced data) or the weight matrix must be provided via the
\texttt{W} argument (for areal data).

In the following subsections, we use the point-referenced \texttt{moss}
data, an \texttt{sf} object that contains data on heavy metals in mosses
near a mining road in Alaska. We view the first few rows of
\texttt{moss} by running

\begin{verbatim}
R> moss
\end{verbatim}

\begin{verbatim}
Simple feature collection with 365 features and 7 fields
Geometry type: POINT
Dimension:     XY
Bounding box:  xmin: -445884.1 ymin: 1929616 xmax: -383656.8 ymax: 2061414
Projected CRS: NAD83 / Alaska Albers
# A tibble: 365 x 8
   sample field_dup lab_rep year  sideroad log_dist2road log_Zn
   <fct>  <fct>     <fct>   <fct> <fct>            <dbl>  <dbl>
 1 001PR  1         1       2001  N                 2.68   7.33
 2 001PR  1         2       2001  N                 2.68   7.38
 3 002PR  1         1       2001  N                 2.54   7.58
 4 003PR  1         1       2001  N                 2.97   7.63
 5 004PR  1         1       2001  N                 2.72   7.26
 6 005PR  1         1       2001  N                 2.76   7.65
 7 006PR  1         1       2001  S                 2.30   7.59
 8 007PR  1         1       2001  N                 2.78   7.16
 9 008PR  1         1       2001  N                 2.93   7.19
10 009PR  1         1       2001  N                 2.79   8.07
# ... with 355 more rows, and 1 more variable:
#   geometry <POINT [m]>
\end{verbatim}

\noindent We can learn more about \texttt{moss} by running
\texttt{help("moss",\ "spmodel")}, and we can visualize the distribution
of log zinc concentration in \texttt{moss} (Fig 1) by running

\begin{verbatim}
R> ggplot(moss, aes(color = log_Zn)) +
+   geom_sf(size = 2) +
+   scale_color_viridis_c() +
+   theme_gray(base_size = 14)
\end{verbatim}

\textbf{Fig 1.Distribution of log zinc concentration in the moss data.}

\noindent Log zinc concentration can be viewed interactively in
\texttt{mapview} by running

\begin{verbatim}
R> mapview(moss, zcol = "log_Zn")
\end{verbatim}

\hypertarget{estimation}{%
\subsection{Estimation}\label{estimation}}

Generally the covariance parameters (\(\boldsymbol{\theta}\)) and fixed
effects (\(\boldsymbol{\beta}\)) of the spatial linear model require
estimation. The default estimation method in \texttt{spmodel} is
restricted maximum likelihood {[}26--28{]}. Maximum likelihood
estimation is also available. For point-referenced data, semivariogram
weighted least squares {[}29{]} and semivariogram composite likelihood
{[}30{]} are additional estimation methods. The estimation method is
chosen using the \texttt{estmethod} argument.

We estimate parameters of a spatial linear model regressing log zinc
concentration (\texttt{log\_Zn}) on log distance to a haul road
(\texttt{log\_dist2road}) using an exponential spatial covariance
function by running

\begin{verbatim}
R> spmod <- splm(log_Zn ~ log_dist2road, moss, spcov_type = "exponential")
\end{verbatim}

\noindent We summarize the model fit by running

\begin{verbatim}
R> summary(spmod)
\end{verbatim}

\begin{verbatim}

Call:
splm(formula = log_Zn ~ log_dist2road, data = moss, spcov_type = "exponential")

Residuals:
    Min      1Q  Median      3Q     Max 
-2.6801 -1.3606 -0.8103 -0.2485  1.1298 

Coefficients (fixed):
              Estimate Std. Error z value Pr(>|z|)    
(Intercept)    9.76825    0.25216   38.74   <2e-16 ***
log_dist2road -0.56287    0.02013  -27.96   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Pseudo R-squared: 0.683

Coefficients (exponential spatial covariance):
       de        ie     range 
3.595e-01 7.897e-02 8.237e+03 
\end{verbatim}

\noindent The fixed effects coefficient table contains estimates,
standard errors, z-statistics, and asymptotic p-values for each fixed
effect. From this table, we notice there is evidence that mean log zinc
concentration significantly decreases with distance from the haul road
(p-value \textless{} 2e-16). We see the fixed effect estimates by
running

\begin{verbatim}
R> coef(spmod)
\end{verbatim}

\begin{verbatim}
  (Intercept) log_dist2road 
    9.7682525    -0.5628713 
\end{verbatim}

\noindent The model summary also contains the exponential spatial
covariance parameter estimates, which we can view by running

\begin{verbatim}
R> coef(spmod, type = "spcov")
\end{verbatim}

\begin{verbatim}
          de           ie        range       rotate        scale 
3.595316e-01 7.896824e-02 8.236712e+03 0.000000e+00 1.000000e+00 
attr(,"class")
[1] "exponential"
\end{verbatim}

\noindent The dependent random error variance (\(\sigma^2_{de}\)) is
estimated to be approximately 0.36 and the independent random error
variance (\(\sigma^2_{ie}\)) is estimated to be approximately 0.079. The
range (\(\phi\)) is estimated to be approximately 8,237. The effective
range is the distance at which the spatial covariance is approximately
zero. For the exponential covariance, the effective range is \(3\phi\).
This means that observations whose distance is greater than 24,711
meters are approximately uncorrelated. The \texttt{rotate} and
\texttt{scale} parameters affect the modeling of anisotropy, which we
discuss later. By default, \texttt{rotate} and \texttt{scale} are
assumed to be zero and one, respectively, which means that anisotropy is
not modeled (i.e., the spatial covariance is assumed isotropic, or
independent of direction). We visualize the fitted spatial covariance
function (Fig 2) by running

\begin{verbatim}
R> plot(spmod, which = 7)
\end{verbatim}

\textbf{Fig 2. Empirical spatial covariance of the fitted model.} The
open circle at a distance of zero represents the
\(\sigma^2_{de} + \sigma^2_{ie}\). The solid line at positive distances
represents \(\sigma^2_{de} \mathbf{R}\) at a particular distance.

\hypertarget{model-fit-statistics}{%
\subsection{Model-Fit Statistics}\label{model-fit-statistics}}

The quality of model fit can be assessed using a variety of statistics
readily available in \texttt{spmodel}. The first model-fit statistic we
consider is the pseudo R-squared. The pseudo R-squared is a
generalization of the classical R-squared from non-spatial linear models
that quantifies the proportion of variability in the data explained by
the fixed effects. The pseudo R-squared is defined as \begin{equation*}
PR2 = 1 - \frac{\mathcal{D}_{\boldsymbol{\hat{\beta}}}}{\mathcal{D}_{\boldsymbol{\hat{\mu}}}},
\end{equation*} where \(\mathcal{D}_{\boldsymbol{\hat{\beta}}}\) is the
deviance of the fitted model with all explanatory variables and
\(\mathcal{D}_{\boldsymbol{\hat{\mu}}}\) is the deviance of the fitted
model with only an intercept. We compute the pseudo R-squared by running

\begin{verbatim}
R> pseudoR2(spmod)
\end{verbatim}

\begin{verbatim}
[1] 0.6829687
\end{verbatim}

\noindent Roughly 68\% of the variability in log zinc is explained by
log distance from the road. The pseudo R-squared can be adjusted to
account for the number of explanatory variables using the
\texttt{adjust} argument. Pseudo R-squared (and the adjusted version) is
most helpful for comparing models that have the same covariance
structure.

The next two model-fit statistics we consider are the AIC and AICc that
{[}31{]} derive for spatial data. The AIC and AICc evaluate the fit of a
model with a penalty for the number of parameters estimated. This
penalty balances model fit and model parsimony. Lower AIC and AICc
indicate a better balance of model fit and parsimony. The AICc is a
correction to AIC that is better suited for small sample sizes. As the
sample size increases, AIC and AICc converge.

The AIC and AICc are given by \begin{equation*}\label{eq:sp_aic}
  \begin{split}
    \text{AIC} & = -2\ell(\hat{\boldsymbol{\Theta}}) + 2(|\hat{\boldsymbol{\Theta}}|) \\
    \text{AICc} & = -2\ell(\hat{\boldsymbol{\Theta}}) + 2n(|\hat{\boldsymbol{\Theta}}|) / (n - |\hat{\boldsymbol{\Theta}}| - 1),
  \end{split}
\end{equation*} where \(\ell(\hat{\boldsymbol{\Theta}})\) is the
log-likelihood of the data evaluated at the estimated parameter vector
\(\hat{\boldsymbol{\Theta}}\) that maximized
\(\ell(\boldsymbol{\Theta})\), \(|\hat{\boldsymbol{\Theta}}|\) is the
cardinality of \(\hat{\boldsymbol{\Theta}}\), and \(n\) is the sample
size. For maximum likelihood,
\(\hat{\boldsymbol{\Theta}} = \{\hat{\boldsymbol{\theta}}, \hat{\boldsymbol{\beta}}\}\),
and for restricted maximum likelihood,
\(\hat{\boldsymbol{\Theta}} = \{\hat{\boldsymbol{\theta}}\}\). There are
some nuances to consider when comparing AIC across models: AIC
comparisons between a model fit using restricted maximum likelihood and
a model fit using maximum likelihood are meaningless, as the models are
fit with different likelihoods; and AIC comparisons between models fit
using restricted maximum likelihood are only valid when the models have
the same fixed effect structure; AIC comparisons between models fit
using maximum likelihood are valid even when the models have different
fixed effect structures {[}32{]}.

Suppose we want to quantify the difference in model quality between the
spatial model and a non-spatial model using the AIC and AICc criteria.
We fit a non-spatial model (Equation\(~\)\ref{eq:lm}) in
\texttt{spmodel} by running

\begin{verbatim}
R> lmod <- splm(log_Zn ~ log_dist2road, moss, spcov_type = "none")
\end{verbatim}

\noindent This model is equivalent to one fit using \texttt{lm()}. We
compute the spatial AIC and AICc of the spatial model and non-spatial
model by running

\begin{verbatim}
R> AIC(spmod, lmod)
\end{verbatim}

\begin{verbatim}
      df      AIC
spmod  3 373.2089
lmod   1 636.0635
\end{verbatim}

\begin{verbatim}
R> AICc(spmod, lmod)
\end{verbatim}

\begin{verbatim}
      df     AICc
spmod  3 373.2754
lmod   1 636.0745
\end{verbatim}

\noindent The noticeably lower AIC and AICc of of the spatial model
indicate that it is a better fit to the data than the non-spatial model.
Recall that these AIC and AICc comparisons are valid because both models
are fit using restricted maximum likelihood (the default).

Another approach to comparing the fitted models is to perform
leave-one-out cross validation {[}33{]}. In leave-one-out cross
validation, a single observation is removed from the data, the model is
re-fit, and a prediction is made for the held-out observation. Then, a
loss metric like mean-squared-prediction error is computed and used to
evaluate model fit. The lower the mean-squared-prediction error, the
better the model fit. For computational efficiency, leave-one-out cross
validation in \texttt{spmodel} is performed by first estimating
\(\boldsymbol{\theta}\) using all the data and then re-estimating
\(\boldsymbol{\beta}\) for each observation. We perform leave-one-out
cross validation for the spatial and non-spatial model by running

\begin{verbatim}
R> loocv(spmod)
\end{verbatim}

\begin{verbatim}
[1] 0.1110895
\end{verbatim}

\begin{verbatim}
R> loocv(lmod)
\end{verbatim}

\begin{verbatim}
[1] 0.3237897
\end{verbatim}

\noindent The noticeably lower mean-squared-prediction error of the
spatial model indicates that it is a better fit to the data than the
non-spatial model.

\hypertarget{diagnostics}{%
\subsection{Diagnostics}\label{diagnostics}}

In addition to model fit metrics, \texttt{spmodel} provides functions to
compute diagnostic metrics that help assess model assumptions and
identify unusual observations.

An observation is said to have high leverage if its combination of
explanatory variable values is far from the mean vector of the
explanatory variables. For a non-spatial model, the leverage of the
\(i\)th observation is the \(i\)th diagonal element of the hat matrix
given by \begin{equation*}
  \mathbf{H} = \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top .
\end{equation*} For a spatial model, the leverage of the \(i\)th
observation is the \(i\)th diagonal element of the spatial hat matrix
given by \begin{equation*}
  \mathbf{H}^* = (\mathbf{X}^* (\mathbf{X}^{* \top} \mathbf{X})^{-1} \mathbf{X}^{* \top}) ,
\end{equation*} where
\(\mathbf{X}^* = \boldsymbol{\Sigma}^{-1/2}\mathbf{X}\) and
\(\boldsymbol{\Sigma}^{-1/2}\) is the inverse square root of the
covariance matrix, \(\boldsymbol{\Sigma}\) {[}34{]}. The spatial hat
matrix can be viewed as the non-spatial hat matrix applied to
\(\mathbf{X}^*\) instead of \(\mathbf{X}\). We compute the hat values
(leverage) by running

\begin{verbatim}
R> hatvalues(spmod)
\end{verbatim}

\noindent Larger hat values indicate more leverage, and observations
with large hat values may be unusual and warrant further investigation.

The fitted value of an observation is the estimated mean response given
the observation's explanatory variable values and the model fit:
\begin{equation*}
  \hat{\mathbf{y}} = \mathbf{X} \hat{\boldsymbol{\beta}}.
\end{equation*} We compute the fitted values by running

\begin{verbatim}
R> fitted(spmod)
\end{verbatim}

\noindent Fitted values for the spatially dependent random errors
(\(\boldsymbol{\tau}\)), spatially independent random errors
(\(\boldsymbol{\epsilon}\)), and random effects can also be obtained via
\texttt{fitted()} by changing the \texttt{type} argument.

The residuals measure each response's deviation from its fitted value.
The response residuals are given by \begin{equation*}
  \mathbf{e}_{r} = \mathbf{y} - \hat{\mathbf{y}}.
\end{equation*} We compute the response residuals of the spatial model
by running

\begin{verbatim}
R> residuals(spmod)
\end{verbatim}

\noindent The response residuals are typically not directly checked for
linear model assumptions, as they have covariance closely resembling the
covariance of \(\mathbf{y}\). Pre-multiplying the residuals by
\(\boldsymbol{\Sigma}^{-1/2}\) yields the Pearson residuals {[}35{]}:
\begin{equation*}
  \mathbf{e}_{p} = \boldsymbol{\Sigma}^{-1/2}\mathbf{e}_{r}.
\end{equation*} When the model is correct, the Pearson residuals have
mean zero, variance approximately one, and are uncorrelated. We compute
the Pearson residuals of the spatial model by running

\begin{verbatim}
R> residuals(spmod, type = "pearson")
\end{verbatim}

\noindent The covariance of \(\mathbf{e}_{p}\) is
\((\mathbf{I} - \mathbf{H}^*)\), which is approximately \(\mathbf{I}\)
for large sample sizes. Explicitly dividing \(\mathbf{e}_{p}\) by the
respective diagonal element of \((\mathbf{I} - \mathbf{H}^*)\) yields
the standardized residuals {[}35{]}: \begin{equation*}
  \mathbf{e}_{s} = \frac{\mathbf{e}_{p}}{\sqrt{(1 - \text{diag}(\mathbf{H}^*))}},
\end{equation*} where \(\text{diag}(\mathbf{H}^*)\) denotes the diagonal
of \(\mathbf{H}^*\). We compute the standardized residuals of the
spatial model by running

\begin{verbatim}
R> residuals(spmod, type = "standardized")
\end{verbatim}

\noindent or

\begin{verbatim}
R> rstandard(spmod)
\end{verbatim}

\noindent When the model is correct, the standardized residuals have
mean zero, variance one, and are uncorrelated.

It is common to check linear model assumptions through visualizations.
We can visualize the standardized residuals vs fitted values by running

\begin{verbatim}
R> plot(spmod, which = 1) # figure omitted
\end{verbatim}

\noindent When the model is correct, the standardized residuals should
be evenly spread around zero with no discernible pattern. We can
visualize a normal QQ-plot of the standardized residuals by running

\begin{verbatim}
R> plot(spmod, which = 2) # figure omitted
\end{verbatim}

\noindent When the standardized residuals are normally distributed, they
should closely follow the normal QQ-line.

An observation is said to be influential if its omission has a large
impact on model fit. Typically, this is measured using Cook's distance
{[}36{]}. For the non-spatial model, the Cook's distance of the \(i\)th
observation is denoted \(\mathbf{D}\) and given by \begin{equation*}
  \mathbf{D} = \mathbf{e}_{s}^2 \frac{\text{diag}(\mathbf{H})}{p(1 - \text{diag}(\mathbf{H}))},
\end{equation*} where \(p\) is the dimension of \(\boldsymbol{\beta}\)
(the number of fixed effects).

For a spatial model, the Cook's distance of the \(i\)th observation is
denoted \(\mathbf{D}^*\) and given by \begin{equation*}
  \mathbf{D}^* = \mathbf{e}_{s}^2 \frac{\text{diag}(\mathbf{H}^*)}{p(1 - \text{diag}(\mathbf{H}^*))} .
\end{equation*} A larger Cook's distance indicates more influence, and
observations with large Cook's distance values may be unusual and
warrant further investigation. We compute Cook's distance by running

\begin{verbatim}
R> cooks.distance(spmod)
\end{verbatim}

\noindent The Cook's distance versus leverage (hat values) can be
visualized by running

\begin{verbatim}
R> plot(spmod, which = 6) # figure omitted
\end{verbatim}

Though we described the model diagnostics in this subsection using
\(\boldsymbol{\Sigma}\), generally the covariance parameters are
estimated and \(\boldsymbol{\Sigma}\) is replaced with
\(\boldsymbol{\hat{\Sigma}}\).

\hypertarget{the-broom-functions-tidy-glance-and-augment}{%
\subsection{\texorpdfstring{The broom functions: \texttt{tidy()},
\texttt{glance()}, and
\texttt{augment()}}{The broom functions: tidy(), glance(), and augment()}}\label{the-broom-functions-tidy-glance-and-augment}}

The \texttt{tidy()}, \texttt{glance()}, and \texttt{augment()} functions
from the broom \textbf{\textsf{R}} package {[}37{]} provide convenient
output for many of the model fit and diagnostic metrics discussed in the
previous two sections. The \texttt{tidy()} function returns a tidy
tibble of the coefficient table from \texttt{summary()}:

\begin{verbatim}
R> tidy(spmod)
\end{verbatim}

\begin{verbatim}
# A tibble: 2 x 5
  term          estimate std.error statistic p.value
  <chr>            <dbl>     <dbl>     <dbl>   <dbl>
1 (Intercept)      9.77     0.252       38.7       0
2 log_dist2road   -0.563    0.0201     -28.0       0
\end{verbatim}

\noindent This tibble format makes it easy to pull out the coefficient
names, estimates, standard errors, z-statistics, and p-values from the
\texttt{summary()} output.

The \texttt{glance()} function returns a tidy tibble of model-fit
statistics:

\begin{verbatim}
R> glance(spmod)
\end{verbatim}

\begin{verbatim}
# A tibble: 1 x 9
      n     p  npar value   AIC  AICc logLik deviance pseudo.r.squared
  <int> <dbl> <int> <dbl> <dbl> <dbl>  <dbl>    <dbl>            <dbl>
1   365     2     3  367.  373.  373.  -184.      363            0.683
\end{verbatim}

The \texttt{glances()} function is an extension of \texttt{glance()}
that can be used to look at many models simultaneously:

\begin{verbatim}
R> glances(spmod, lmod)
\end{verbatim}

\begin{verbatim}
# A tibble: 2 x 10
  model     n     p  npar value   AIC  AICc logLik deviance
  <chr> <int> <dbl> <int> <dbl> <dbl> <dbl>  <dbl>    <dbl>
1 spmod   365     2     3  367.  373.  373.  -184.     363 
2 lmod    365     2     1  634.  636.  636.  -317.     363.
# ... with 1 more variable: pseudo.r.squared <dbl>
\end{verbatim}

Finally, the \texttt{augment()} function augments the original data with
model diagnostics:

\begin{verbatim}
R> augment(spmod)
\end{verbatim}

\begin{verbatim}
Simple feature collection with 365 features and 7 fields
Geometry type: POINT
Dimension:     XY
Bounding box:  xmin: -445884.1 ymin: 1929616 xmax: -383656.8 ymax: 2061414
Projected CRS: NAD83 / Alaska Albers
# A tibble: 365 x 8
   log_Zn log_dist2road .fitted .resid    .hat  .cooksd .std.resid
 *  <dbl>         <dbl>   <dbl>  <dbl>   <dbl>    <dbl>      <dbl>
 1   7.33          2.68    8.26 -0.928 0.102   0.112        -1.48 
 2   7.38          2.68    8.26 -0.880 0.0101  0.000507     -0.316
 3   7.58          2.54    8.34 -0.755 0.0170  0.000475     -0.236
 4   7.63          2.97    8.09 -0.464 0.0137  0.000219      0.178
 5   7.26          2.72    8.24 -0.977 0.0177  0.00515      -0.762
 6   7.65          2.76    8.21 -0.568 0.0147  0.000929     -0.355
 7   7.59          2.30    8.47 -0.886 0.0170  0.00802      -0.971
 8   7.16          2.78    8.20 -1.05  0.0593  0.0492       -1.29 
 9   7.19          2.93    8.12 -0.926 0.00793 0.000451     -0.337
10   8.07          2.79    8.20 -0.123 0.0265  0.00396       0.547
# ... with 355 more rows, and 1 more variable: geometry <POINT [m]>
\end{verbatim}

\noindent By default, only the columns of \texttt{data} used to fit the
model are returned alongside the diagnostics. All original columns of
\texttt{data} are returned by setting \texttt{drop} to \texttt{FALSE}.
\texttt{augment()} is especially powerful when the data are an
\texttt{sf} object because model diagnostics can be easily visualized
spatially. For example, we could subset the augmented object so that it
only includes observations whose standardized residuals have absolute
values greater than some cutoff and then map them.

\hypertarget{an-areal-data-example}{%
\subsection{An Areal Data Example}\label{an-areal-data-example}}

Next we use the \texttt{seal} data, an \texttt{sf} object that contains
the log of the estimated harbor-seal trends from abundance data across
polygons in Alaska, to provide an example of fitting a spatial linear
model for areal data using \texttt{spautor()}. We view the first few
rows of \texttt{seal} by running

\begin{verbatim}
R> seal
\end{verbatim}

\begin{verbatim}
Simple feature collection with 62 features and 1 field
Geometry type: POLYGON
Dimension:     XY
Bounding box:  xmin: 913618.8 ymin: 1007542 xmax: 1116002 ymax: 1145054
Projected CRS: NAD83 / Alaska Albers
# A tibble: 62 x 2
   log_trend                    geometry
       <dbl>               <POLYGON [m]>
 1  NA       ((1035002 1054710, 1035002~
 2  -0.282   ((1037002 1039492, 1037006~
 3  -0.00121 ((1070158 1030216, 1070185~
 4   0.0354  ((1054906 1034826, 1054931~
 5  -0.0160  ((1025142 1056940, 1025184~
 6   0.0872  ((1026035 1044623, 1026037~
 7  -0.266   ((1100345 1060709, 1100287~
 8   0.0743  ((1030247 1029637, 1030248~
 9  NA       ((1043093 1020553, 1043097~
10  -0.00961 ((1116002 1024542, 1116002~
# ... with 52 more rows
\end{verbatim}

\noindent We can learn more about the data by running
\texttt{help("seal",\ "spmodel")}.

We can visualize the distribution of log seal trends in the
\texttt{seal} data (Fig 3) by running

\begin{verbatim}
R> ggplot(seal, aes(fill = log_trend)) +
+   geom_sf(size = 0.75) +
+   scale_fill_viridis_c() +
+   theme_bw(base_size = 14)
\end{verbatim}

\textbf{Fig 3. Distribution of log seal trends in the seal data.}
Polygons are gray if seal trends are missing.

\noindent Log trends can be viewed interactively in \texttt{mapview} by
running

\begin{verbatim}
R> mapview(seal, zcol = "log_trend")
\end{verbatim}

The gray polygons denote areas where the log trend is missing. These
missing areas need to be kept in the data while fitting the model to
preserve the overall neighborhood structure.

We estimate parameters of a spatial autoregressive model for log seal
trends (\texttt{log\_trend}) using an intercept-only model with a
conditional autoregressive (CAR) spatial covariance by running

\begin{verbatim}
R> sealmod <- spautor(log_trend ~ 1, seal, spcov_type = "car")
\end{verbatim}

If a weight matrix is not provided to \texttt{spautor()}, it is
calculated internally using queen contiguity. Recall that queen
contiguity defines two observations as neighbors if they share at least
one common boundary. If at least one observation has no neighbors, the
\texttt{extra} parameter is estimated, which quantifies variability
among observations without neighbors. By default, \texttt{spautor()}
uses row standardization {[}24{]} and assumes an independent error
variance (\texttt{ie}) of zero.

We summarize, tidy, glance at, and augment the fitted model by running

\begin{verbatim}
R> summary(sealmod)
\end{verbatim}

\begin{verbatim}

Call:
spautor(formula = log_trend ~ 1, data = seal, spcov_type = "car")

Residuals:
     Min       1Q   Median       3Q      Max 
-0.34443 -0.10405  0.04422  0.07349  0.20487 

Coefficients (fixed):
            Estimate Std. Error z value Pr(>|z|)   
(Intercept) -0.07102    0.02495  -2.846  0.00443 **
---
Signif. codes:  
0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Coefficients (car spatial covariance):
     de   range   extra 
0.03261 0.41439 0.02221 
\end{verbatim}

\begin{verbatim}
R> tidy(sealmod)
\end{verbatim}

\begin{verbatim}
# A tibble: 1 x 5
  term        estimate std.error statistic p.value
  <chr>          <dbl>     <dbl>     <dbl>   <dbl>
1 (Intercept)  -0.0710    0.0250     -2.85 0.00443
\end{verbatim}

\begin{verbatim}
R> glance(sealmod)
\end{verbatim}

\begin{verbatim}
# A tibble: 1 x 9
      n     p  npar value   AIC  AICc logLik deviance pseudo.r.squared
  <int> <dbl> <int> <dbl> <dbl> <dbl>  <dbl>    <dbl>            <dbl>
1    34     1     3 -36.9 -30.9 -30.1   18.4     32.9                0
\end{verbatim}

\begin{verbatim}
R> augment(sealmod)
\end{verbatim}

\begin{verbatim}
Simple feature collection with 34 features and 6 fields
Geometry type: POLYGON
Dimension:     XY
Bounding box:  xmin: 980001.5 ymin: 1010815 xmax: 1116002 ymax: 1145054
Projected CRS: NAD83 / Alaska Albers
# A tibble: 34 x 7
   log_trend .fitted  .resid   .hat .cooksd .std.resid
 *     <dbl>   <dbl>   <dbl>  <dbl>   <dbl>      <dbl>
 1  -0.282   -0.0710 -0.211  0.0179 0.0233      -1.14 
 2  -0.00121 -0.0710  0.0698 0.0699 0.0412       0.767
 3   0.0354  -0.0710  0.106  0.0218 0.0109       0.705
 4  -0.0160  -0.0710  0.0550 0.0343 0.00633      0.430
 5   0.0872  -0.0710  0.158  0.0229 0.0299       1.14 
 6  -0.266   -0.0710 -0.195  0.0280 0.0493      -1.33 
 7   0.0743  -0.0710  0.145  0.0480 0.0818       1.30 
 8  -0.00961 -0.0710  0.0614 0.0143 0.00123      0.293
 9  -0.182   -0.0710 -0.111  0.0131 0.0155      -1.09 
10   0.00351 -0.0710  0.0745 0.0340 0.0107       0.561
# ... with 24 more rows, and 1 more variable:
#   geometry <POLYGON [m]>
\end{verbatim}

Note that for \texttt{spautor()} models, the \texttt{ie} spatial
covariance parameter is assumed zero by default (and omitted from the
\texttt{summary()} output). This default behavior can be overridden by
specifying \texttt{ie} in the \texttt{spcov\_initial} argument to
\texttt{spautor()}. Also note that the pseudo R-squared is zero because
there are no explanatory variables in the model (i.e., it is an
intercept-only model).

\hypertarget{sec:prediction}{%
\section{Prediction}\label{sec:prediction}}

In this section, we show how to use \texttt{predict()} to perform
spatial prediction (also called Kriging) in \texttt{spmodel}. We will
fit a model using the point-referenced \texttt{sulfate} data, an
\texttt{sf} object that contains sulfate measurements in the
conterminous United States, and make predictions for each location in
the point-referenced \texttt{sulfate\_preds} data, an \texttt{sf} object
that contains locations in the conterminous United States at which to
predict sulfate.

We first visualize the distribution of the sulfate data (Fig 4A) by
running

\begin{verbatim}
R> ggplot(sulfate, aes(color = sulfate)) +
+   geom_sf(size = 2.5) +
+   scale_color_viridis_c(limits = c(0, 45)) +
+   theme_gray(base_size = 18)
\end{verbatim}

\textbf{Fig 4. Distribution of observed sulfate and sulfate predictions
in the conterminous United States.} In A (top), observed sulfate is
visualized. In B (bottom), sulfate predictions are visualized.

We then fit a spatial linear model for sulfate using an intercept-only
model with a spherical spatial covariance function by running

\begin{verbatim}
R> sulfmod <- splm(sulfate ~ 1, sulfate, spcov_type = "spherical")
\end{verbatim}

\noindent Then we obtain best linear unbiased predictions (Kriging
predictions) using \texttt{predict()}. The \texttt{newdata} argument
contains the locations at which to predict, and we store the predictions
as a new variable in \texttt{sulfate\_preds} called \texttt{preds} by
running

\begin{verbatim}
R> sulfate_preds$preds <- predict(sulfmod, newdata = sulfate_preds)
\end{verbatim}

\noindent We can visualize the model predictions (Fig 4B) by running

\begin{verbatim}
R> ggplot(sulfate_preds, aes(color = preds)) +
+   geom_sf(size = 2.5) +
+   scale_color_viridis_c(limits = c(0, 45)) +
+   theme_gray(base_size = 18)
\end{verbatim}

It is important to properly specify the \texttt{newdata} object when
running \texttt{predict()}. If explanatory variables were used to fit
the model, the same explanatory variables must be included in
\texttt{newdata} with the same names as they have in \texttt{data}.
Additionally, if an explanatory variable is categorical or a factor, the
values of this variable in \texttt{newdata} must also be values in
\texttt{data} (e.g., if a categorical variable with values \texttt{"A"},
and \texttt{"B"} was used to fit the model, the corresponding variable
in \texttt{newdata} cannot have a value \texttt{"C"}). If \texttt{data}
is a \texttt{data.frame}, coordinates must be included in
\texttt{newdata} with the same names as they have in \texttt{data}. If
\texttt{data} is an \texttt{sf} object, coordinates must be included in
\texttt{newdata} with the same geometry name as they have in
\texttt{data}. When using projected coordinates, the projection for
\texttt{newdata} should be the same as the projection for \texttt{data}.

Prediction standard errors are returned by setting the \texttt{se.fit}
argument to \texttt{TRUE}:

\begin{verbatim}
R> predict(sulfmod, newdata = sulfate_preds, se.fit = TRUE)
\end{verbatim}

\noindent The \texttt{interval} argument determines the type of interval
returned. If \texttt{interval} is \texttt{"none"} (the default), no
intervals are returned. If \texttt{interval} is \texttt{"prediction"},
then \texttt{100\ *\ level}\% prediction intervals are returned (the
default is 95\% prediction intervals):

\begin{verbatim}
R> predict(sulfmod, newdata = sulfate_preds, interval = "prediction")
\end{verbatim}

\noindent If \texttt{interval} is \texttt{"confidence"}, the predictions
are instead the estimated mean given each observation's explanatory
variable values (i.e., fitted values) and the corresponding
\texttt{100\ *\ level}\% confidence intervals are returned:

\begin{verbatim}
R> predict(sulfmod, newdata = sulfate_preds, interval = "confidence")
\end{verbatim}

\noindent The \texttt{predict()} output structure changes based on
\texttt{interval} and \texttt{se.fit}. For more details, run
\texttt{help("predict.spmodel",\ "spmodel")}.

Previously we used the \texttt{augment()} function to augment
\texttt{data} with model diagnostics. We can also use \texttt{augment()}
to augment \texttt{newdata} with predictions, standard errors, and
intervals. We remove the model predictions from \texttt{sulfate\_preds}
before showing how \texttt{augment()} is used to obtain the same
predictions by running

\begin{verbatim}
R> sulfate_preds$preds <- NULL
\end{verbatim}

\noindent We then view the first few rows of \texttt{sulfate\_preds}
augmented with a 90\% prediction interval by running

\begin{verbatim}
R> augment(
+   sulfmod,
+   newdata = sulfate_preds,
+   interval = "prediction",
+   level = 0.90
+ )
\end{verbatim}

\begin{verbatim}
Simple feature collection with 100 features and 3 fields
Geometry type: POINT
Dimension:     XY
Bounding box:  xmin: -2283774 ymin: 582930.5 xmax: 1985906 ymax: 3037173
Projected CRS: NAD83 / Conus Albers
# A tibble: 100 x 4
   .fitted .lower .upper            geometry
 *   <dbl>  <dbl>  <dbl>         <POINT [m]>
 1    1.40  -5.33   8.14  (-1771413 1752976)
 2   24.5   18.2   30.8    (1018112 1867127)
 3    8.99   2.36  15.6  (-291256.8 1553212)
 4   16.4    9.92  23.0    (1274293 1267835)
 5    4.91  -1.56  11.4  (-547437.6 1638825)
 6   26.7   20.4   33.0    (1445080 1981278)
 7    3.00  -3.65   9.66  (-1629090 3037173)
 8   14.3    7.97  20.6    (1302757 1039534)
 9    1.49  -5.08   8.06  (-1429838 2523494)
10   14.4    7.97  20.8    (1131970 1096609)
# ... with 90 more rows
\end{verbatim}

\noindent Here \texttt{.fitted} represents the predictions,
\texttt{.lower} represents the lower bound of the 90\% prediction
intervals, and \texttt{.upper} represents the upper bound of the 95\%
prediction intervals.

An alternative (but equivalent) approach can be used for model fitting
and prediction that circumvents the need to keep \texttt{data} and
\texttt{newdata} as separate objects. Suppose that observations
requiring prediction are stored in \texttt{data} as missing
(\texttt{NA}) values. We can add a column of missing values to
\texttt{sulfate\_preds} and then bind it together with \texttt{sulfate}
by running

\begin{verbatim}
R> sulfate_preds$sulfate <- NA
R> sulfate_with_NA <- rbind(sulfate, sulfate_preds)
\end{verbatim}

\noindent We can then fit a spatial linear model by running

\begin{verbatim}
R> sulfmod_with_NA <- splm(
+   sulfate ~ 1,
+   sulfate_with_NA,
+   spcov_type = "spherical"
+ )
\end{verbatim}

\noindent The missing values are ignored for model-fitting but stored in
\texttt{sulfmod\_with\_NA} as \texttt{newdata}:

\begin{verbatim}
R> sulfmod_with_NA$newdata
\end{verbatim}

\begin{verbatim}
Simple feature collection with 100 features and 1 field
Geometry type: POINT
Dimension:     XY
Bounding box:  xmin: -2283774 ymin: 582930.5 xmax: 1985906 ymax: 3037173
Projected CRS: NAD83 / Conus Albers
First 10 features:
    sulfate                  geometry
198      NA  POINT (-1771413 1752976)
199      NA   POINT (1018112 1867127)
200      NA POINT (-291256.8 1553212)
201      NA   POINT (1274293 1267835)
202      NA POINT (-547437.6 1638825)
203      NA   POINT (1445080 1981278)
204      NA  POINT (-1629090 3037173)
205      NA   POINT (1302757 1039534)
206      NA  POINT (-1429838 2523494)
207      NA   POINT (1131970 1096609)
\end{verbatim}

\noindent We can then predict the missing values by running

\begin{verbatim}
R> predict(sulfmod_with_NA)
\end{verbatim}

\noindent The call to \texttt{predict()} finds in
\texttt{sulfmod\_with\_NA} the \texttt{newdata} object and is equivalent
to

\begin{verbatim}
R> predict(sulfmod_with_NA, newdata = sulfmod_with_NA$newdata)
\end{verbatim}

We can also use \texttt{augment()} to make the predictions for the data
set with missing values by running

\begin{verbatim}
R> augment(sulfmod_with_NA, newdata = sulfmod_with_NA$newdata)
\end{verbatim}

\begin{verbatim}
Simple feature collection with 100 features and 2 fields
Geometry type: POINT
Dimension:     XY
Bounding box:  xmin: -2283774 ymin: 582930.5 xmax: 1985906 ymax: 3037173
Projected CRS: NAD83 / Conus Albers
# A tibble: 100 x 3
   sulfate .fitted            geometry
 *   <dbl>   <dbl>         <POINT [m]>
 1      NA    1.40  (-1771413 1752976)
 2      NA   24.5    (1018112 1867127)
 3      NA    8.99 (-291256.8 1553212)
 4      NA   16.4    (1274293 1267835)
 5      NA    4.91 (-547437.6 1638825)
 6      NA   26.7    (1445080 1981278)
 7      NA    3.00  (-1629090 3037173)
 8      NA   14.3    (1302757 1039534)
 9      NA    1.49  (-1429838 2523494)
10      NA   14.4    (1131970 1096609)
# ... with 90 more rows
\end{verbatim}

\noindent Unlike \texttt{predict()}, \texttt{augment()} explicitly
requires the \texttt{newdata} argument be specified in order to obtain
predictions. Omitting \texttt{newdata} (e.g., running
\texttt{augment(sulfmod\_with\_NA)}) returns model diagnostics, not
predictions.

For areal data models fit with \texttt{spautor()}, predictions cannot be
computed at locations that were not incorporated in the neighborhood
structure used to fit the model. Thus, predictions are only possible for
observations in \texttt{data} whose response values are missing
(\texttt{NA}), as their locations are incorporated into the neighborhood
structure. For example, we make predictions of log seal trends at the
missing polygons from Fig 3 by running

\begin{verbatim}
R> predict(sealmod)
\end{verbatim}

\noindent We can also use \texttt{augment()} to make the predictions:

\begin{verbatim}
R> augment(sealmod, newdata = sealmod$newdata)
\end{verbatim}

\begin{verbatim}
Simple feature collection with 28 features and 2 fields
Geometry type: POLYGON
Dimension:     XY
Bounding box:  xmin: 913618.8 ymin: 1007542 xmax: 1115097 ymax: 1132682
Projected CRS: NAD83 / Alaska Albers
# A tibble: 28 x 3
   log_trend .fitted
 *     <dbl>   <dbl>
 1        NA -0.113 
 2        NA -0.0108
 3        NA -0.0608
 4        NA -0.0383
 5        NA -0.0730
 6        NA -0.0556
 7        NA -0.0968
 8        NA -0.0716
 9        NA -0.0776
10        NA -0.0647
# ... with 18 more rows, and 1 more
#   variable: geometry <POLYGON [m]>
\end{verbatim}

\hypertarget{sec:advfeatures}{%
\section{Advanced Features}\label{sec:advfeatures}}

\texttt{spmodel} offers several advanced features for fitting spatial
linear models. We briefly discuss some of these features next using the
\texttt{moss} data and some simulated data. Technical details for each
advanced feature can be seen by running
\texttt{vignette("technical",\ "spmodel")}.

\hypertarget{sec:spcov_init}{%
\subsection{Fixing Spatial Covariance Parameters}\label{sec:spcov_init}}

We may desire to fix specific spatial covariance parameters at a
particular value. Perhaps some parameter value is known, for example. Or
perhaps we want to compare nested models where a reduced model uses a
fixed parameter value while the full model estimates the parameter.
Fixing spatial covariance parameters while fitting a model is possible
using the \texttt{spcov\_initial} argument to \texttt{splm()} and
\texttt{spautor()}. The \texttt{spcov\_initial} argument takes an
\texttt{spcov\_initial} object (run
\texttt{help("spcov\_initial",\ "spmodel")} for more).
\texttt{spcov\_initial} objects can also be used to specify initial
values used during optimization, even if they are not assumed to be
fixed. By default, \texttt{spmodel} uses a grid search to find suitable
initial values to use during optimization.

As an example, suppose our goal is to compare a model with an
exponential covariance and dependent error variance, independent error
variance, and range parameter to a similar model that instead assumes
the independent random error variance parameter (nugget) is zero. First,
the \texttt{spcov\_initial} object is specified for the latter model:

\begin{verbatim}
R> init <- spcov_initial("exponential", ie = 0, known = "ie")
R> init
\end{verbatim}

\begin{verbatim}
$initial
ie 
 0 

$is_known
  ie 
TRUE 

attr(,"class")
[1] "exponential"
\end{verbatim}

\noindent The \texttt{init} output shows that the \texttt{ie} parameter
has an initial value of zero that is assumed to be known. Next the model
is fit:

\begin{verbatim}
R> spmod_red <- splm(log_Zn ~ log_dist2road, moss, spcov_initial = init)
\end{verbatim}

\noindent Notice that because the \texttt{spcov\_initial} object
contains information about the spatial covariance type, the
\texttt{spcov\_type} argument is not required when
\texttt{spcov\_initial} is provided. We can use \texttt{glances()} to
glance at both models:

\begin{verbatim}
R> glances(spmod, spmod_red)
\end{verbatim}

\begin{verbatim}
# A tibble: 2 x 10
  model         n     p  npar value   AIC  AICc logLik deviance
  <chr>     <int> <dbl> <int> <dbl> <dbl> <dbl>  <dbl>    <dbl>
1 spmod       365     2     3  367.  373.  373.  -184.     363 
2 spmod_red   365     2     2  378.  382.  382.  -189.     374.
# ... with 1 more variable: pseudo.r.squared <dbl>
\end{verbatim}

The lower AIC and AICc of the full model compared to the reduced model
indicates that the independent random error variance is important to the
model. A likelihood ratio test comparing the full and reduced models is
also possible using \texttt{anova()}.

Another application of fixing spatial covariance parameters involves
calculating their profile likelihood confidence intervals {[}38{]}.
Before calculating a profile likelihood confidence interval for
\(\boldsymbol{\Theta}_i\), the \(i\)th element of a general parameter
vector \(\boldsymbol{\Theta}\), it is necessary to obtain
\(-2\ell(\hat{\boldsymbol{\Theta}})\), minus twice the log-likelihood
evaluated at the estimated parameter vector,
\(\hat{\boldsymbol{\Theta}}\). Then a \((1 - \alpha)\)\% profile
likelihood confidence interval is the set of values for
\(\boldsymbol{\Theta}_i\) such that
\(2\ell(\hat{\boldsymbol{\Theta}}) - 2\ell(\hat{\boldsymbol{\Theta}}_{-i}) \leq \chi^2_{1, 1 - \alpha}\),
where \(\ell(\hat{\boldsymbol{\Theta}}_{-i})\) is the value of the
log-likelihood maximized after fixing \(\boldsymbol{\Theta}_i\) and
optimizing over the remaining parameters, \(\boldsymbol{\Theta}_{-i}\),
and \(\chi^2_{1, 1 - \alpha}\) is the \(1 - \alpha\) quantile of a
chi-squared distribution with one degree of freedom. The result follows
from inverting a likelihood ratio test comparing the full model to a
reduced model that fixes the value of \(\boldsymbol{\Theta}_i\). Because
computing profile likelihood confidence intervals requires refitting the
model many times for different fixed values of
\(\boldsymbol{\Theta}_i\), it can be computationally intensive. This
approach can be generalized to yield joint profile likelihood confidence
intervals cases when \(i\) has dimension greater than one.

\hypertarget{fitting-and-predicting-for-multiple-models}{%
\subsection{Fitting and Predicting for Multiple
Models}\label{fitting-and-predicting-for-multiple-models}}

Fitting multiple models is possible with a single call to
\texttt{splm()} or \texttt{spautor()} when \texttt{spcov\_type} is a
vector with length greater than one or \texttt{spcov\_initial} is a list
(with length greater than one) of \texttt{spcov\_initial} objects. We
fit three separate spatial linear models using the exponential spatial
covariance, spherical spatial covariance, and no spatial covariance by
running

\begin{verbatim}
R> spmods <- splm(
+   sulfate ~ 1,
+   sulfate,
+   spcov_type = c("exponential", "spherical", "none")
+ )
\end{verbatim}

\noindent Then \texttt{glances()}is used to glance at each fitted model
object:

\begin{verbatim}
R> glances(spmods)
\end{verbatim}

\begin{verbatim}
# A tibble: 3 x 10
  model           n     p  npar value   AIC  AICc logLik deviance
  <chr>       <int> <dbl> <int> <dbl> <dbl> <dbl>  <dbl>    <dbl>
1 spherical     197     1     3 1137. 1143. 1143.  -569.     196.
2 exponential   197     1     3 1140. 1146. 1146.  -570.     196.
3 none          197     1     1 1448. 1450. 1450.  -724.     196 
# ... with 1 more variable: pseudo.r.squared <dbl>
\end{verbatim}

\noindent And \texttt{predict()} is used to predict \texttt{newdata}
separately fo each fitted model object:

\begin{verbatim}
R> predict(spmods, newdata = sulfate_preds)
\end{verbatim}

Currently, \texttt{glances()} and \texttt{predict()} are the only
\texttt{spmodel} generic functions that operate on an object that
contains multiple model fits. Generic functions that operate on
individual models can still be called when the argument is an individual
model object. For example, we can compute the AIC of the model fit using
the exponential covariance function by running

\begin{verbatim}
R> AIC(spmods$exponential)
\end{verbatim}

\begin{verbatim}
[1] 1145.824
\end{verbatim}

\hypertarget{random-effects}{%
\subsection{Random Effects}\label{random-effects}}

Non-spatial random effects incorporate additional sources of variability
into model fitting. They are accommodated in \texttt{spmodel} using
similar syntax as for random effects in the nlme {[}32{]} and lme4
{[}39{]} \textbf{\textsf{R}} packages. Random effects are specified via
a formula passed to the \texttt{random} argument. Next, we show two
examples that incorporate random effects into the spatial linear model
using the \texttt{moss} data.

The first example explores random intercepts for the \texttt{sample}
variable. The \texttt{sample} variable indexes each unique location,
which can have replicate observations due to field duplicates
(\texttt{field\_dup}) and lab replicates (\texttt{lab\_rep}). There are
365 observations in \texttt{moss} at 318 unique locations, which means
that 47 observations in \texttt{moss} are either field duplicates or lab
replicates. It is likely that the repeated observations at a location
are correlated with one another. We can incorporate this
repeated-observation correlation by creating a random intercept for each
level of \texttt{sample}. We model the random intercepts for
\texttt{sample} by running

\begin{verbatim}
R> rand1 <- splm(
+   log_Zn ~ log_dist2road,
+   moss,
+   spcov_type = "exponential",
+   random = ~ sample
+ )
\end{verbatim}

\noindent Note that \texttt{\textasciitilde{}\ sample} is shorthand for
\texttt{\textasciitilde{}\ (1\ \textbar{}\ sample)}, which is more
explicit notation that indicates random intercepts for each level of
\texttt{sample}.

The second example adds a random intercept for \texttt{year}, which
creates extra correlation for observations within a year. It also adds a
random slope for \texttt{log\_dist2road} within \texttt{year}, which
lets the effect of \texttt{log\_dist2road} vary between years. We fit
this model by running

\begin{verbatim}
R> rand2 <- splm(
+   log_Zn ~ log_dist2road,
+   moss,
+   spcov_type = "exponential",
+   random = ~ sample + (log_dist2road | year)
+ )
\end{verbatim}

\noindent Note that
\texttt{\textasciitilde{}\ sample\ +\ (log\_dist2road\ \textbar{}\ year)}
is shorthand for
\texttt{\textasciitilde{}\ (1\ \textbar{}\ sample)\ +\ (log\_dist2road\ \textbar{}\ year)}.
If only random slopes within a year are desired (and no random
intercepts), a \texttt{-\ 1} is given to the relevant portion of the
formula: \texttt{(log\_dist2road\ -\ 1\ \textbar{}\ year)}. When there
is more than one term in \texttt{random}, each term must be surrounded
by parentheses (recall that the random intercept shorthand automatically
includes relevant parentheses).

We can compare the AIC of all three models by running

\begin{verbatim}
R> AIC(spmod, rand1, rand2)
\end{verbatim}

\begin{verbatim}
      df      AIC
spmod  3 373.2089
rand1  4 343.1021
rand2  6 201.8731
\end{verbatim}

\noindent The \texttt{rand2} model has the lowest AIC.

It is possible to fix random effect variances using the
\texttt{randcov\_initial} argument, and \texttt{randcov\_initial} can
also be used to set initial values for optimization.

\hypertarget{partition-factors}{%
\subsection{Partition Factors}\label{partition-factors}}

A partition factor is a variable that allows observations to be
uncorrelated when they are from different levels of the partition
factor. Partition factors are specified in \texttt{spmodel} by providing
a formula with a single variable to the \texttt{partition\_factor}
argument. Suppose that for the \texttt{moss} data, we would like
observations in different years (\texttt{year}) to be uncorrelated. We
fit a model that treats year as a partition factor by running

\begin{verbatim}
R> part <- splm(
+   log_Zn ~ log_dist2road,
+   moss,
+   spcov_type = "exponential",
+   partition_factor = ~ year
+ )
\end{verbatim}

\hypertarget{sec:anisotropy}{%
\subsection{Anisotropy}\label{sec:anisotropy}}

An isotroptic spatial covariance function (for point-referenced data)
behaves similarly in all directions (i.e., is independent of direction)
as a function of distance. An anisotropic covariance function does not
behave similarly in all directions as a function of distance. Consider
the spatial covariance imposed by an eastward-moving wind pattern. A
one-unit distance in the x-direction likely means something different
than a one-unit distance in the y-direction. Fig 5 shows ellipses for an
isotropic and anisotropic covariance function centered at the origin (a
distance of zero). The black outline of each ellipse is a level curve of
equal correlation. The left ellipse (a circle) represents an isotropic
covariance function. The distance at which the correlation between two
observations lays on the level curve is the same in all directions. The
right ellipse represents an anisotropic covariance function. The
distance at which the correlation between two observations lays on the
level curve is different in different directions.

\textbf{Fig 5. Ellipses for an isotropic and anisotropic covariance
function centered at the origin.} In A (left), the isotropic covariance
function is visualized. In B (right), the anisotropic covariance
function is visualized. The black outline of each ellipse is a level
curve of equal correlation.

Accounting for anisotropy involves a rotation and scaling of the
x-coordinates and y-coordinates such that the spatial covariance
function that uses these transformed distances is isotropic. We use the
\texttt{anisotropy} argument to \texttt{splm()} to fit a model with
anisotropy by running

\begin{verbatim}
R> spmod_anis <- splm(
+   log_Zn ~ log_dist2road,
+   moss,
+   spcov_type = "exponential",
+   anisotropy = TRUE
+ )
R> summary(spmod_anis)
\end{verbatim}

\begin{verbatim}

Call:
splm(formula = log_Zn ~ log_dist2road, data = moss, spcov_type = "exponential", 
    anisotropy = TRUE)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.5279 -1.2239 -0.7202 -0.1921  1.1659 

Coefficients (fixed):
              Estimate Std. Error z value Pr(>|z|)    
(Intercept)    9.54798    0.22291   42.83   <2e-16 ***
log_dist2road -0.54601    0.01855  -29.44   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Pseudo R-squared: 0.7048

Coefficients (exponential spatial covariance):
       de        ie     range    rotate     scale 
3.561e-01 6.812e-02 8.732e+03 2.435e+00 4.753e-01 
attr(,"class")
[1] "exponential"
\end{verbatim}

\noindent The \texttt{rotate} parameter is between zero and \(\pi\)
radians and represents the angle of a clockwise rotation of the ellipse
such that the major axis of the ellipse is the new x-axis and the minor
axis of the ellipse is the new y-axis. The \texttt{scale} parameter is
between zero and one and represents the ratio of the distance between
the origin and the edge of the ellipse along the minor axis to the
distance between the origin and the edge of the ellipse along the major
axis. The transformation that turns an anisotropic ellipse into an
isotropic one (i.e., a circle) requires rotating the coordinates
clockwise by \texttt{rotate} and then scaling them the reciprocal of
\texttt{scale}. The transformed coordinates are then used instead of the
original coordinates to compute distances and spatial covariances.

Note that specifying an initial value for \texttt{rotate} that is
different from zero, specifying an initial value for \texttt{scale} that
is different from one, or assuming either \texttt{rotate} or
\texttt{scale} are unknown in \texttt{spcov\_initial} will cause
\texttt{splm()} to fit a model with anisotropy (and will override
\texttt{anisotropy\ =\ FALSE}). Estimating anisotropy parameters is only
possible for maximum likelihood and restricted maximum likelihood
estimation, but fixed anisotropy parameters can be accommodated for
semivariogram weighted least squares or semivariogram composite
likelihood estimation. Also note that anisotropy is not relevant for
areal data because the spatial covariance function depends on a
neighborhood structure instead of distances between locations.

\hypertarget{sec:sim_data}{%
\subsection{Simulating Spatial Data}\label{sec:sim_data}}

The \texttt{sprnorm()} function is used to simulate normal (Gaussian)
spatial data. To use \texttt{sprnorm()}, the \texttt{spcov\_params()}
function is used to create an \texttt{spcov\_params} object. The
\texttt{spcov\_params()} function requires the spatial covariance type
and parameter values. We create an \texttt{spcov\_params} object by
running

\begin{verbatim}
R> sim_params <- spcov_params("exponential", de = 5, ie = 1, range = 0.5)
\end{verbatim}

We set a reproducible seed and then simulate data at 3000 random
locations in the unit square using the spatial covariance parameters in
\texttt{sim\_params} by running

\begin{verbatim}
R> set.seed(0)
R> n <- 3000
R> x <- runif(n)
R> y <- runif(n)
R> coords <- tibble::tibble(x, y)
R> resp <- sprnorm(
+   sim_params,
+   data = coords,
+   xcoord = x,
+   ycoord = y
+ )
R> sim_data <- tibble::tibble(coords, resp)
\end{verbatim}

\noindent We can visualize the simulated data (Fig 6A) by running

\begin{verbatim}
R> ggplot(sim_data, aes(x = x, y = y, color = resp)) +
+   geom_point(size = 1.5) +
+   scale_color_viridis_c(limits = c(-7, 7)) + 
+   theme_gray(base_size = 18)
\end{verbatim}

\noindent There is noticeable spatial patterning in the response
variable (\texttt{resp}). The default mean in \texttt{sprnorm()} is zero
for all observations, though a mean vector can be provided using the
\texttt{mean} argument. The default number of samples generated in
\texttt{sprnorm()} is one, though this can be changed using the
\texttt{samples} argument. Because \texttt{sim\_data} is a
\texttt{tibble} (\texttt{data.frame}) and not an \texttt{sf} object, the
columns in \texttt{sim\_data} representing the x-coordinates and
y-coordinates must be provided to \texttt{sprnorm()}.

\textbf{Fig 6. Observed data and big data predictions at unobserved
locations.} In A (top), spatial data are simulated in the unit square. A
spatial linear model is fit using the default big data approximation for
model-fitting. In B (bottom), predictions are made using the fitted
model and the default big data approximation for prediction.

Note that the output from \texttt{coef(object,\ type\ =\ "spcov")} is a
\texttt{spcov\_params} object. This is useful we want to simulate data
given the estimated spatial covariance parameters from a fitted model.
Random effects are incorporated into simulation via the
\texttt{randcov\_params} argument.

\hypertarget{big-data}{%
\subsection{Big Data}\label{big-data}}

The computational cost associated with model fitting is exponential in
the sample size for all estimation methods. For maximum likelihood and
restricted maximum likelihood, the computational cost of estimating
\(\boldsymbol{\theta}\) is cubic. For semivariogram weighted least
squares and semivariogram composite likelihood, the computational cost
of estimating \(\boldsymbol{\theta}\) is quadratic. The computational
cost associated with estimating \(\boldsymbol{\beta}\) and prediction is
cubic in the model-fitting sample size, regardless of estimation method.
Typically, samples sizes approaching 10,000 make the computational cost
of model fitting and prediction infeasible, which necessitates the use
of big data methods. \texttt{spmodel} offers big data methods for model
fitting of point-referenced data via the \texttt{local} argument to
\texttt{splm()}. The method is capable of quickly fitting models with
hundreds of thousands to millions of observations. Because of the
neighborhood structure of areal data, the big data methods used for
point-referenced data do not apply to areal data. Thus, there is no big
data method for areal data or \texttt{local} argument to
\texttt{spautor()}, so model fitting sample sizes cannot be too large.
\texttt{spmodel} offers big data methods for prediction of
point-referenced data or areal data via the \texttt{local} argument to
\texttt{predict()}, capable of quickly predicting hundreds of thousands
to millions of observations rather quickly.

To show how to use \texttt{spmodel} for big data estimation and
prediction, we use the \texttt{sim\_data} data from the previous
subsection. Because \texttt{sim\_data} is a \texttt{tibble}
(\texttt{data.frame}) and not an \texttt{sf} object, the columns in
\texttt{data} representing the x-coordinates and y-coordinates must be
explicitly provided to \texttt{splm()}.

\hypertarget{model-fitting}{%
\subsubsection{Model-fitting}\label{model-fitting}}

\texttt{spmodel} uses a ``local indexing'' approximation for big data
model fitting of point-referenced data. Observations are first assigned
an index. Then for the purposes of model fitting, observations with
different indexes are assumed uncorrelated. Assuming observations with
different indexes are uncorrelated induces sparsity in the covariance
matrix, which greatly reduces the computational time of operations that
involve the covariance matrix.

The \texttt{local} argument to \texttt{splm()} controls the big data
options. \texttt{local} is a list with several arguments. The arguments
to the \texttt{local} list control the method used to assign the
indexes, the number of observations with the same index, the number of
unique indexes, adjustments to the covariance matrix of
\(\hat{\boldsymbol{\beta}}\), whether or not to use parallel processing,
and if parallel processing is used, the number of cores.

Big data are most simply accommodated by setting \texttt{local} to
\texttt{TRUE}. This is shorthand for
\texttt{local\ =\ list(method\ =\ "random",\ size\ =\ 50,\ var\_adjust\ =\ "theoretical",\ parallel\ =\ FALSE)},
which randomly assigns observations to index groups, ensures each index
group has approximately 50 observations, uses the theoretically-correct
covariance adjustment, and does not use parallel processing.

\begin{verbatim}
R> local1 <- splm(
+   resp ~ 1,
+   sim_data,
+   spcov_type = "exponential",
+   xcoord = x,
+   ycoord = y,
+   local = TRUE
+ )
R> summary(local1)
\end{verbatim}

\begin{verbatim}

Call:
splm(formula = resp ~ 1, data = sim_data, spcov_type = "exponential", 
    xcoord = x, ycoord = y, local = TRUE)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.0356 -1.3514 -0.1468  1.2842  6.5381 

Coefficients (fixed):
            Estimate Std. Error z value Pr(>|z|)
(Intercept)   -1.021      0.699   -1.46    0.144

Coefficients (exponential spatial covariance):
    de     ie  range 
2.8724 0.9735 0.2644 
\end{verbatim}

Instead of using \texttt{local\ =\ TRUE}, we can explicitly set
\texttt{local}. For example, we can fit a model using using k-means
clustering {[}40{]} on the x-coordinates and y-coordinates to create 60
groups (clusters), use the pooled variance adjustment, and use parallel
processing with two cores by running

\begin{verbatim}
R> local2_list <- list(
+   method = "kmeans",
+   groups = 60,
+   var_adjust = "pooled",
+   parallel = TRUE,
+   ncores = 2
+ )
R> local2 <- splm(
+   resp ~ 1,
+   sim_data,
+   spcov_type = "exponential",
+   xcoord = x,
+   ycoord = y,
+   local = local2_list
+ )
R> summary(local2)
\end{verbatim}

\begin{verbatim}

Call:
splm(formula = resp ~ 1, data = sim_data, spcov_type = "exponential", 
    xcoord = x, ycoord = y, local = local2_list)

Residuals:
     Min       1Q   Median       3Q      Max 
-4.98801 -1.30386 -0.09927  1.33176  6.58567 

Coefficients (fixed):
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  -1.0683     0.1759  -6.073 1.25e-09 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Coefficients (exponential spatial covariance):
    de     ie  range 
2.5434 0.9907 0.2312 
\end{verbatim}

Likelihood-based statistics like \texttt{AIC()}, \texttt{AICc()},
\texttt{logLik()}, and \texttt{deviance()} should not be used to compare
a model fit with a big data approximation to a model fit without a big
data approximation, as the two approaches maximize different
likelihoods.

\hypertarget{sec:predict}{%
\subsubsection{Prediction}\label{sec:predict}}

For point-referenced data, \texttt{spmodel} uses a ``local
neighborhood'' approximation for big data prediction. Each prediction is
computed using a subset of the observed data instead of all of the
observed data. Before further discussing big data prediction, we
simulate 1000 locations in the unit square requiring prediction:

\begin{verbatim}
R> n_pred <- 1000
R> x <- runif(n_pred)
R> y <- runif(n_pred)
R> sim_preds <- tibble::tibble(x = x, y = y)
\end{verbatim}

The \texttt{local} argument to \texttt{predict()} controls the big data
options. \texttt{local} is a list with several arguments. The arguments
to the \texttt{local} list control the method used to subset the
observed data, the number of observations in each subset, whether or not
to use parallel processing, and if parallel processing is used, the
number of cores.

The simplest way to accommodate big data prediction is to set
\texttt{local} to \texttt{TRUE}. This is shorthand for
\texttt{local\ =\ list(method\ =\ "covariance",\ size\ =\ 50,\ parallel\ =\ FALSE)},
which implies that for each location requiring prediction, only the 50
observations in the data most correlated with it are used in the
computation, and parallel processing is not used. Using the
\texttt{local1} fitted model, we store these predictions as a variable
called \texttt{preds} in the \texttt{sim\_preds} data by running

\begin{verbatim}
R> sim_preds$preds <- predict(local1, newdata = sim_preds, local = TRUE)
\end{verbatim}

\noindent The predictions are visualized (Fig 6B) by running

\begin{verbatim}
R> ggplot(sim_preds, aes(x = x, y = y, color = preds)) +
+   geom_point(size = 1.5) +
+   scale_color_viridis_c(limits = c(-7, 7)) + 
+   theme_gray(base_size = 18)
\end{verbatim}

\noindent They display a similar pattern as the observed data.

Instead of using \texttt{local\ =\ TRUE}, we can explicitly set
\texttt{local}:

\begin{verbatim}
R> pred_list <- list(
+   method = "distance",
+   size = 30,
+   parallel = TRUE,
+   ncores = 2
+ )
R> predict(local1, newdata = sim_preds, local = pred_list)
\end{verbatim}

\noindent This code implies that uniquely for each location requiring
prediction, only the 30 observations in the data closest to it (in terms
of Euclidean distance) are used in the computation and parallel
processing is used with two cores.

For areal data, no local neighborhood approximation exists because of
the data's underlying neighborhood structure. Thus, all of the data must
be used to compute predictions and by consequence, \texttt{method} and
\texttt{size} are not components of the \texttt{local} list. The only
components of the \texttt{local} list for areal data are
\texttt{parallel} and \texttt{ncores}.

\hypertarget{sec:discussion}{%
\section{Discussion}\label{sec:discussion}}

\texttt{spmodel} is a novel, relevant contribution used to fit,
summarize, and predict for a variety of spatial statistical models.
Spatial linear models for point-referenced data (i.e., geostatistical
models) are fit using the \texttt{splm()} function. Spatial linear
models for areal data (i.e., autoregressive models) are fit using the
\texttt{spautor()} function. Both functions use a common framework and
syntax structure. Several model-fit statistics and diagnostics are
available. The broom functions \texttt{tidy()} and \texttt{glance()} are
used to tidy and glance at a fitted model. The broom function
\texttt{augment()} is used to augment \texttt{data} with model
diagnostics and augment \texttt{newdata} with predictions. Several
advanced features are available to accommodate fixed covariance
parameter values, random effects, partition factors, anisotropy,
simulating data, and big data approximations for model fitting and
prediction.

We appreciate feedback from users regarding \texttt{spmodel}, and we
have several plans to add new features to \texttt{spmodel} in the
future. To learn more about \texttt{spmodel} or provide feedback, please
visit our GitHub repository at \url{https://github.com/USEPA/spmodel}.

\hypertarget{data-and-code-availability}{%
\section*{Data and Code Availability}\label{data-and-code-availability}}
\addcontentsline{toc}{section}{Data and Code Availability}

This manuscript has a supplementary \(\textbf{\textsf{R}}\) package that
contains all of the data, code, and figures used in its creation. The
supplementary \textbf{\textsf{R}} package is hosted on GitHub and
available at \url{https://github.com/michaeldumelle/spmodel.manuscript}.

\hypertarget{author-contributions}{%
\section*{Author Contributions}\label{author-contributions}}
\addcontentsline{toc}{section}{Author Contributions}

\begin{itemize}
\item
  Michael Dumelle: Conceptualization, Data Curation, Formal Analysis,
  Investigation, Methodology, Project Administration, Resources,
  Software, Supervision, Validation, Visualization, Writing
\item
  Matt Higham: Formal Analysis, Investigation, Methodology, Project
  Administration, Resources, Software, Supervision, Validation,
  Visualization, Writing
\item
  Jay M. Ver Hoef: Data Curation, Formal Analysis, Investigation,
  Methodology, Project Administration, Resources, Software, Supervision,
  Validation, Visualization, Writing
\end{itemize}

\hypertarget{acknowledgements}{%
\section*{Acknowledgements}\label{acknowledgements}}
\addcontentsline{toc}{section}{Acknowledgements}

The views expressed in this manuscript are those of the authors and do
not necessarily represent the views or policies of the U.S.
Environmental Protection Agency or the National Oceanic and Atmospheric
Administration. Any mention of trade names, products, or services does
not imply an endorsement by the U.S. government, the U.S. Environmental
Protection Agency, or the National Oceanic and Atmospheric
Administration. The U.S. Environmental Protection Agency and the
National Oceanic and Atmospheric Administration do not endorse any
commercial products, services or enterprises.

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-brus1997random}{}%
1. Brus D, De Gruijter J. Random sampling or geostatistical modelling?
Choosing between design-based and model-based sampling strategies for
soil (with discussion). Geoderma. 1997;80: 1--44.

\leavevmode\hypertarget{ref-cressie1993statistics}{}%
2. Cressie N. Statistics for spatial data. John Wiley \& Sons; 1993.

\leavevmode\hypertarget{ref-banerjee2003hierarchical}{}%
3. Banerjee S, Carlin BP, Gelfand AE. Hierarchical modeling and analysis
for spatial data. Chapman; Hall/CRC; 2003.

\leavevmode\hypertarget{ref-schabenberger2017statistical}{}%
4. Schabenberger O, Gotway CA. Statistical methods for spatial data
analysis: Texts in statistical science. Chapman; Hall/CRC; 2017.

\leavevmode\hypertarget{ref-nychka2021fields}{}%
5. Nychka D, Furrer R, Paige J, Sain S. Fields: Tools for spatial data.
Boulder, CO, USA: University Corporation for Atmospheric Research; 2021.
Available: \url{https://github.com/dnychka/fieldsRPackage}

\leavevmode\hypertarget{ref-zammitmangion2021FRK}{}%
6. Zammit-Mangion A, Cressie N. FRK: An R package for spatial and
spatio-temporal prediction with large datasets. Journal of Statistical
Software. 2021;98: 1--48.

\leavevmode\hypertarget{ref-ribiero2022geoR}{}%
7. Ribeiro Jr PJ, Diggle P, Christensen O, Schlather M, Bivand R, Ripley
B. GeoR: Analysis of geostatistical data. 2022. Available:
\url{https://CRAN.R-project.org/package=geoR}

\leavevmode\hypertarget{ref-guiness2021gpgp}{}%
8. Guiness J, Katzfuss M, Fahmy Y. GpGp: Fast Gaussian process
computation using Vecchia's approximation. 2021. Available:
\url{https://CRAN.R-project.org/package=GpGp}

\leavevmode\hypertarget{ref-pebesma2004gstat}{}%
9. Pebesma E. Multivariable geostatistics in S: The gstat package.
Computers \& Geosciences. 2004;30: 683--691.

\leavevmode\hypertarget{ref-nychka2016latticekrig}{}%
10. Nychka D, Hammerling D, Sain S, Lenssen N. LatticeKrig:
Multiresolution Kriging based on Markov random fields. Boulder, CO, USA:
University Corporation for Atmospheric Research; 2016.
doi:\href{https://doi.org/10.5065/D6HD7T1R}{10.5065/D6HD7T1R}

\leavevmode\hypertarget{ref-lindgren2015bayesian}{}%
11. Lindgren F, Rue H. Bayesian spatial modelling with r-inla. Journal
of statistical software. 2015;63: 1--25.

\leavevmode\hypertarget{ref-rstan2023rstan}{}%
12. Stan Development Team. RStan: The R interface to Stan. 2023.
Available: \url{https://mc-stan.org/}

\leavevmode\hypertarget{ref-venables2002S}{}%
13. Venables WN, Ripley BD. Modern applied statistics with S. Fourth.
New York: Springer; 2002. Available:
\url{https://www.stats.ox.ac.uk/pub/MASS4/}

\leavevmode\hypertarget{ref-finley2007spbayes}{}%
14. Finley AO, Banerjee S, Carlin BP. spBayes: An R package for
univariate and multivariate hierarchical point-referenced spatial
models. Journal of Statistical Software. 2007;19: 1--24. Available:
\url{https://www.jstatsoft.org/article/view/v019i04}

\leavevmode\hypertarget{ref-finley2002spnngp}{}%
15. Finley AO, Datta A, Banerjee S. spNNGP R package for nearest
neighbor Gaussian process models. Journal of Statistical Software.
2022;103: 1--40.
doi:\href{https://doi.org/10.18637/jss.v103.i05}{10.18637/jss.v103.i05}

\leavevmode\hypertarget{ref-burkner2017brms}{}%
16. Brkner P-C. Brms: An r package for bayesian multilevel models using
stan. Journal of statistical software. 2017;80: 1--28.

\leavevmode\hypertarget{ref-lee2013carbayes}{}%
17. Lee D. CARBayes: An R package for Bayesian spatial modeling with
conditional autoregressive priors. Journal of Statistical Software.
2013;55: 1--24. Available:
\url{https://www.jstatsoft.org/htaccess.php?volume=55\&type=i\&issue=13}

\leavevmode\hypertarget{ref-adin2022bigdm}{}%
18. Adin A, Orozco-Acosta E, Vicente G, Ugarte MD. BigDM: Scalable
bayesian disease mapping models for high-dimensional data. 2022.
Available: \url{https://github.com/spatialstatisticsupna/bigDM}

\leavevmode\hypertarget{ref-ronnegard2010hglm}{}%
19. Ronnegard L, Shen X, Alam M. Hglm: A package for fitting
hierarchical generalized linear models. The R Journal. 2010;2: 20--28.
Available:
\url{https://journal.r-project.org/archive/2010-2/RJournal_2010-2_Roennegaard~et~al.pdf}

\leavevmode\hypertarget{ref-wickham2016ggplot2}{}%
20. Wickham H. Ggplot2: Elegant graphics for data analysis.
Springer-Verlag New York; 2016. Available:
\url{https://ggplot2.tidyverse.org}

\leavevmode\hypertarget{ref-appelhans2022mapview}{}%
21. Appelhans T, Detsch F, Reudenbach C, Woellauer S. Mapview:
Interactive viewing of spatial data in R. 2022. Available:
\url{https://CRAN.R-project.org/package=mapview}

\leavevmode\hypertarget{ref-tobler1970computer}{}%
22. Tobler WR. A computer movie simulating urban growth in the Detroit
region. Economic geography. 1970;46: 234--240.

\leavevmode\hypertarget{ref-anselin2010geoda}{}%
23. Anselin L, Syabri I, Kho Y. GeoDa: An introduction to spatial data
analysis. Handbook of applied spatial analysis. Springer; 2010. pp.
73--89.

\leavevmode\hypertarget{ref-ver2018spatial}{}%
24. Ver Hoef JM, Peterson EE, Hooten MB, Hanks EM, Fortin M-J. Spatial
autoregressive models for statistical inference from ecological data.
Ecological Monographs. 2018;88: 36--59.

\leavevmode\hypertarget{ref-pebesma2018sf}{}%
25. Pebesma E. Simple features for R: standardized support for spatial
vector data. The R Journal. 2018;10: 439--446.
doi:\href{https://doi.org/10.32614/RJ-2018-009}{10.32614/RJ-2018-009}

\leavevmode\hypertarget{ref-patterson1971recovery}{}%
26. Patterson D, Thompson R. Recovery of inter-block information when
block sizes are unequal. Biometrika. 1971;58: 545--554.

\leavevmode\hypertarget{ref-harville1977maximum}{}%
27. Harville DA. Maximum likelihood approaches to variance component
estimation and to related problems. Journal of the American Statistical
Association. 1977;72: 320--338.

\leavevmode\hypertarget{ref-wolfinger1994computing}{}%
28. Wolfinger R, Tobias R, Sall J. Computing Gaussian likelihoods and
their derivatives for general linear mixed models. SIAM Journal on
Scientific Computing. 1994;15: 1294--1310.

\leavevmode\hypertarget{ref-cressie1985fitting}{}%
29. Cressie N. Fitting variogram models by weighted least squares.
Journal of the international Association for mathematical Geology.
1985;17: 563--586.

\leavevmode\hypertarget{ref-curriero1999composite}{}%
30. Curriero FC, Lele S. A composite likelihood approach to
semivariogram estimation. Journal of Agricultural, biological, and
Environmental statistics. 1999; 9--28.

\leavevmode\hypertarget{ref-hoeting2006model}{}%
31. Hoeting JA, Davis RA, Merton AA, Thompson SE. Model selection for
geostatistical models. Ecological Applications. 2006;16: 87--98.

\leavevmode\hypertarget{ref-pinheiro2006mixed}{}%
32. Pinheiro J, Bates D. Mixed-effects models in S and S-PLUS. Springer
science \& business media; 2006.

\leavevmode\hypertarget{ref-hastie2009elements}{}%
33. Hastie T, Tibshirani R, Friedman JH, Friedman JH. The elements of
statistical learning: Data mining, inference, and prediction. Springer;
2009.

\leavevmode\hypertarget{ref-montgomery2021introduction}{}%
34. Montgomery DC, Peck EA, Vining GG. Introduction to linear regression
analysis. John Wiley \& Sons; 2021.

\leavevmode\hypertarget{ref-myers2012generalized}{}%
35. Myers RH, Montgomery DC, Vining GG, Robinson TJ. Generalized linear
models: With applications in engineering and the sciences. John Wiley \&
Sons; 2012.

\leavevmode\hypertarget{ref-cook1982residuals}{}%
36. Cook RD, Weisberg S. Residuals and influence in regression. New
York: Chapman; Hall; 1982.

\leavevmode\hypertarget{ref-robinson2021broom}{}%
37. Robinson D, Hayes A, Couch S. Broom: Convert statistical objects
into tidy tibbles. 2021. Available:
\url{https://CRAN.R-project.org/package=broom}

\leavevmode\hypertarget{ref-box1964analysis}{}%
38. Box GE, Cox DR. An analysis of transformations. Journal of the Royal
Statistical Society: Series B (Methodological). 1964;26: 211--243.

\leavevmode\hypertarget{ref-bates2015lme4}{}%
39. Bates D, Mchler M, Bolker B, Walker S. Fitting linear mixed-effects
models using lme4. Journal of Statistical Software. 2015;67: 1--48.
doi:\href{https://doi.org/10.18637/jss.v067.i01}{10.18637/jss.v067.i01}

\leavevmode\hypertarget{ref-macqueen1967some}{}%
40. MacQueen J, others. Some methods for classification and analysis of
multivariate observations. Proceedings of the fifth berkeley symposium
on mathematical statistics and probability. Oakland, CA, USA; 1967. pp.
281--297.

\nolinenumbers



\end{document}

