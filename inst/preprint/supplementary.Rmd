---
title: |
  spmodel: Spatial Modeling in **R** -- Supplementary Material
authors:
  - name: Michael Dumelle
    thanks: Corresponding Author
    department: United States
    affiliation: Environmental Protection Agency
    location: 200 SW 35th St, Corvallis, OR, 97333
    email: Dumelle.Michael@epa.gov
  - name: Matt Higham
    department: Department of Math, Computer Science, and Statistics
    affiliation: St. Lawrence University
    location: 23 Romoda Drive, Canton, NY, 13617
    email: mhigham@stlawu.edu
  - name: Jay M. Ver Hoef
    department: National Oceanic and Atmospheric Administration
    affiliation: Alaska Fisheries Science Center
    location: Marine Mammal Laboratory, Seattle, WA, 98115
    email: jay.verhoef@noaa.gov
abstract: |
  Enter the text of your abstract here.
keywords:
  - Spatial covariance
  - Linear Model
  - Autoregressive model
bibliography: references.bib
biblio-style: unsrt
header-includes: |
  \usepackage{amsmath,amsfonts,amssymb}
  \usepackage{bm, bbm}
  \usepackage{mathtools}
  \mathtoolsset{showonlyrefs=true}
output: rticles::arxiv_article
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(prompt=TRUE, echo = FALSE, highlight = FALSE, continue = " + ", comment = "")
options(replace.assign=TRUE, width=90, prompt="R> ")
library(spmodel)
library(ggplot2)
```

# Covariance Functions

# Estimation

## Likelihood-based Estimation

Minus twice a profiled Gaussian log-likelihood, denoted $-2l(\bm{\theta} | \mathbf{y})$ is given by 
\begin{equation}\label{eq:ml-lik}
  -2l(\bm{\theta} | \mathbf{y}) = \ln{|\mathbf{\Sigma}|} + (\mathbf{y} - \mathbf{X} \tilde{\bm{\beta}})^\intercal \mathbf{\Sigma}^{-1} (\mathbf{y} - \mathbf{X} \tilde{\bm{\beta}}) + n \ln{2\pi},
\end{equation}
where $\tilde{\bm{\beta}} = (\mathbf{X}^\intercal \mathbf{\Sigma}^{-1} \mathbf{X})^{-1} \mathbf{X}^\intercal \mathbf{\Sigma}^{-1} \mathbf{y}$. Minimizing Equation$~$\ref{eq:ml-lik} yields $\bm{\hat{\theta}}_{ml}$, the maximum likelihood estimates for $\bm{\theta}$. Then a closed for solution exists for $\bm{\hat{\beta}}_{ml}$, the maximum likelihood estimates for $\bm{\beta}$: $\bm{\hat{\beta}}_{ml} = \tilde{\bm{\beta}}_{ml}$, where $\tilde{\bm{\beta}}_{ml}$ is $\tilde{\bm{\beta}}$ evaluated at $\bm{\hat{\theta}}_{ml}$. Unfortunately $\bm{\hat{\theta}}_{ml}$ can be badly biased for $\bm{\theta}$ (especially for small sample sizes), which impacts the estimation of $\bm{\beta}$ [@patterson1971recovery]. This bias occurs due to the simultaneous estimation of $\bm{\beta}$ and $\bm{\theta}$ To reduce this bias, restricted maximum likelihood estimation (REML) emerged [@patterson1971recovery; @harville1977maximum; @wolfinger1994computing]. It can be shown that integrating $\bm{\beta}$ out of a Gaussian likelihood yields the restricted Gaussian likelihood used in REML estimation. Minus twice a restricted Gaussian log-likelihood, denoted $-2l_R(\bm{\theta} | \mathbf{y})$ is given by 
\begin{equation}\label{eq:reml-lik}
  -2l_R(\bm{\theta} | \mathbf{y}) = -2l(\bm{\theta} | \mathbf{y})  + \ln{|\mathbf{X}^\intercal \mathbf{\Sigma}^{-1} \mathbf{X}|} - p \ln{2\pi} ,
\end{equation}
where $p$ equals the dimension of $\bm{\beta}$. Minimizing Equation$~$\ref{eq:reml-lik} yields $\bm{\hat{\theta}}_{reml}$, the restricted maximum likelihood estimates for $\bm{\theta}$. Then a closed for solution exists for $\bm{\hat{\beta}}_{reml}$, the restricted maximum likelihood estimates for $\bm{\beta}$: $\bm{\hat{\beta}}_{reml} = \tilde{\bm{\beta}}_{reml}$, where $\tilde{\bm{\beta}}_{reml}$ is $\tilde{\bm{\beta}}$ evaluated at $\bm{\hat{\theta}}_{reml}$.

Generally the overall variance, $\sigma^2$, can be profiled out of Equation$~$\ref{eq:ml-lik} and Equation$~$\ref{eq:reml-lik}. This reduces the number of parameters requiring optimization by one, which can dramatically reduce estimation time. For example, profiling $\sigma^2$ out of Equation$~$\ref{eq:ml-lik} yields
\begin{equation}\label{eq:ml-plik}
  -2l^*(\bm{\theta}^* | \mathbf{y}) = \ln{|\mathbf{\Sigma^*}|} + n\ln[(\mathbf{y} - \mathbf{X} \tilde{\bm{\beta}})^\intercal \mathbf{\Sigma}^{-1} (\mathbf{y} - \mathbf{X} \tilde{\bm{\beta}})] + n + n\ln{2\pi / n}.
\end{equation}
After finding $\hat{\bm{\theta}}^*_{ml}$ a closed form solution for $\hat{\sigma}^2_{ml}$ exists: $\hat{\sigma}^2_{ml} = [(\mathbf{y} - \mathbf{X} \bm{\tilde{\beta}})^\intercal \mathbf{\Sigma}^{-1} (\mathbf{y} - \mathbf{X} \tilde{\bm{\beta}})] / n$. Then $\bm{\hat{\theta}}^*_{ml}$ is combined with $\hat{\sigma}^2_{ml}$ to yield $\bm{\hat{\theta}}_{ml}$ and subsequently $\bm{\hat{\beta}}_{ml}$. A similar result holds for REML estimation. Profiling $\sigma^2$ out of Equation$~$\ref{eq:reml-lik} yields
\begin{equation}\label{eq:reml-plik}
  -2l_R^*(\bm{\theta}^* | \mathbf{y}) = \ln{|\mathbf{\Sigma^*}|} + (n - p)\ln[(\mathbf{y} - \mathbf{X} \tilde{\bm{\beta}})^\intercal \mathbf{\Sigma}^{-1} (\mathbf{y} - \mathbf{X} \tilde{\bm{\beta}})] + (n - p) + (n - p)\ln2\pi / (n - p).
\end{equation}
After finding $\hat{\bm{\theta}}^*_{reml}$ a closed form solution for $\hat{\sigma}^2_{reml}$ exists: $\hat{\sigma}^2_{reml} = [(\mathbf{y} - \mathbf{X} \bm{\tilde{\beta}})^\intercal \mathbf{\Sigma}^{-1} (\mathbf{y} - \mathbf{X} \tilde{\bm{\beta}})] / (n - p)$. Then $\bm{\hat{\theta}}^*_{reml}$ is combined with $\hat{\sigma}^2_{reml}$ to yield $\bm{\hat{\theta}}_{reml}$ and subsequently $\bm{\hat{\beta}}_{reml}$.

Both ML and REML estimation rely on the an $n \times n$ covariance matrix inverse. Inverting an $n \times n$ matrix is an enormous computational demand that scales cubically with the sample size. For this reason, ML and REML have historically been unfeasible to implement in their standard form with data larger than a few thousand observations. This motivates the use for the big data approaches outlined in Section$~$(INSERT SECTION).

It is worth noting that the inverses themselves are not strictly needed for estimation (or prediction), though at least their square root is needed. In spmodel, calculating this square root requires a Cholesky decomposition, which still scales cubically with the sample size. Computing the Cholesky decomposition, however, is far more computationally efficient than computing the inverse. To see why only the Cholesky decomposition is needed, recall that the Cholesky decomposition of the covariance matrix $\mathbf{\Sigma}$ is $\mathbf{C}\mathbf{C}^\intercal$, where $\mathbf{C}$ is a lower triangular matrix (so $\mathbf{C}\mathbf{C}^\intercal = \mathbf{\Sigma}$). In the ML and REML likelihoods, $\mathbf{\Sigma}^{-1}$ is not needed on its own, only $\mathbf{X}^\intercal \mathbf{\Sigma}^{-1} \mathbf{X}$ and $\mathbf{X}^\intercal \mathbf{\Sigma}^{-1} \mathbf{y}$ are needed. We can rewrite the $\mathbf{X}^\intercal \mathbf{\Sigma}^{-1} \mathbf{X}$ as $\mathbf{X}^\intercal (\mathbf{C}^\intercal)^{-1} \mathbf{C}^{-1} \mathbf{X} = (\mathbf{C}^{-1} \mathbf{X})^\intercal \mathbf{C}^{-1} \mathbf{X}$. Then $\mathbf{C}^{-1} \mathbf{X}$ is efficiently solved by noticing that $\mathbf{C}^{-1} \mathbf{X} = \mathbf{A}$ for some matrix $\mathbf{A}$ implies $\mathbf{X} = \mathbf{C} \mathbf{A}$. This system can be efficiently solved for $\mathbf{A}$ using linear forward solves (forward substitution). Then $\mathbf{X}^\intercal \mathbf{\Sigma}^{-1} \mathbf{X} = \mathbf{A}^\intercal \mathbf{A}$. A similar approach is used to solve $\mathbf{X}^\intercal \mathbf{\Sigma}^{-1} \mathbf{y}$. Still, using Cholesky decomositions is unfeasible for sample sizes larger than a few thousand observations.

## Semivariogram-based Estimation

An alternative approach to likelihood-based estimation is semivariogram-based estimation. The semivariogram of a constant-mean process $\mathbf{y}$ is the expectation of the squared half-difference between two observations $h$ distance units apart. More formally, the semivariogram is denoted $\gamma(h)$ and defined as
\begin{equation}\label{eq:sv}
  \gamma(h) = \text{E}(y_i - y_j)^2 / 2 ,
\end{equation}
where $||y_i - y_j||_2 = h$ (the Euclidean distance). When the process $\mathbf{y}$ is second-order stationary, the semivariogram and covariance function are intimately connected: $\gamma(h) = \text{Cov}(0) - \text{Cov}(h)$, where $\text{Cov}(0)$ is the covariance function evaluated at 0 (which is the overall variance, $\sigma^2$) and $\text{Cov}(h)$is the covariance function evaluated at $h$. Both semivariogram approaches described next are more computationally efficient than ML or REML because their major computational burden (calculations based on pairs) scale the squared sample size (i.e., not the cubed sample size).

### Weighted Least Squares

The empirical semivariogram is a moment-based estimate of the semivariogram denoted by $\hat{\gamma}(h)$ and defined as
\begin{equation}\label{eq:esv}
  \hat{\gamma}(h) = \frac{1}{2|N(h)|} \sum_{N(h)} (y_i - y_j)^2, 
\end{equation}
where $N(h)$ is the set of observations in $\mathbf{y}$ that are $h$ units apart (distance classes) and $|N(h)|$ is the cardinality of $N(h)$ [@cressie1993statistics]. Often the set $N(h)$ contains observations that are $h \pm \alpha$ apart -- this approach is known as "binning" the empirical semivariogram. Typically, only certain $h$ considered when constructing Equation$~$\ref{eq:esv} -- a commonly used cutoff is to ignore $h$ larger than half the maximum distance in the domain. One criticism of the empirical semivariogram is that distance bins and cutoffs tend to be artitrarily chosen (i.e., not chosen according to some statistical criteria).

Equation$~$\eqref{eq:esv} is viewed as the average squared half-distance between two observations in $\mathbf{y}$. @cressie1985fitting proposed estimating $\bm{\theta}$ by minimizing an objective function that involves $\gamma{h}$ and $\hat{\gamma}(h)$ and is based on a weighted least squares criterion. This criterion is defined as
\begin{equation}\label{eq:svwls}
  \sum_i w_i [\hat{\gamma}(h)_i - \gamma(h)_i]^2,
\end{equation}
where $w_i$, $\hat{\gamma}(h)_i$, and $\gamma(h)_i$ are the weights, empirical semivariogram, and semivariogram for the $i$th distance class. Minimizing Equation$~$\eqref{eq:svwls} yields $\bm{\hat{\theta}}_{wls}$, the semivariogram weighted least squares estimates of $\bm{\theta}$. After estimating $\bm{\theta}$, $\bm{\beta}$ estimates are constructed using (empirical) generalized least squares: $\bm{\hat{\beta}}_{wls} = (\mathbf{X}^\intercal \hat{\mathbf{\Sigma}}^{-1} \mathbf{X})^{-1} \mathbf{X}^\intercal \hat{\mathbf{\Sigma}}^{-1} \mathbf{y}$, where $\hat{\mathbf{\Sigma}}^{-1}$ is $\mathbf{\Sigma}$ evaluated at $\bm{\hat{\theta}}_{wls}$. 

@cressie1985fitting recommends setting the $w_i$ in Equation$~$\eqref{eq:svwls} as $w_i = |N(h)| / \gamma(h)_i^2$, which gives more weights to distance classes with more observations ($|N(h)|$) and semivariances at shorter distances ($1 / \gamma(h)_i^2$). The default in spmodel is to use these $w_i$ -- the type of $w_i$ is changed via the \texttt{weights} argument to \texttt{splm()}. Table$~$\ref{tab:weights} contains all $w_i$ available in spmodel. 
\begin{table}\label{tab:weights}
  \centering
  \begin{tabular}{c|c|c}
  \hline
  $w_i$ Name & $w_i$ Form & \texttt{weight = } \\
  Cressie & $|N(h)| / \gamma(h)_i^2$ & \texttt{"cressie"} \\
  Cressie (Denominator) Root & $|N(h)| / \gamma(h)_i$ & \texttt{"cressie-droot"} \\
  Cressie No Pairs & $1 / \gamma(h)_i^2$ & \texttt{"cressie-nopairs"} \\
  Cressie (Denominator) Root No Pairs & $1 / \gamma(h)_i$ & \texttt{"cressie-droot-nopairs"} \\
  Pairs & $|N(h)|$ & \texttt{"pairs"} \\
  Pairs Inverse Distance & $|N(h)| / h^2$ & \texttt{"pairs-invd"} \\
  Pairs Inverse (Root) Distance & $|N(h)| / h$ & \texttt{"pairs-invsd"} \\
  Ordinary Least Squares & 1 & \texttt{ols}
  \end{tabular}
  \caption{spmodel table weights}
\end{table}
Additionally, the number of $N(h)$ classes and maximum distance for $h$ are specified by passing the \texttt{bins} and \texttt{cutoff} arguments to \texttt{splm()} (these arguments are passed via \texttt{...} to \texttt{esv()}). The default value for \texttt{bins} is 15 and the default value for the maximum $h$ is half the maximum distance of the spatial domain's bounding box.

Recall that the semivariogram is defined for a constant-mean process. Generally, $\mathbf{y}$ does not necessarily have a constant mean. So the empirical semivariogram and $\bm{\hat{\theta}}_{wls}$ are typically constructed using the residuals from an ordinary least squares regression of $\mathbf{y}$ on $\mathbf{X}$ -- these residuals are assumed to have mean zero.

## Optimization

### Composite Likelihood

The composite likelihood approach involves constructing a likelihood based on conditional or marginal events for which log-likelihoods are available and then adding together these individual components. Composite likelihoods are attractive because they behave very similar to likelihoods but are easier to handle, both from a theoretical and a computational perspective. @curriero1999composite derive a particular composite likelihood for estimating semivariogram parameters. The negative log of this composite likelihood, denoted $\text{CL}(h)$, is given by
\begin{equation}\label{eq:svcl}
  \text{CL}(h) = \sum_{i = 1}^{n - 1} \sum_{j > i} \left( \frac{(y_i - y_j)^2}{2\gamma(h)} + \ln(\gamma(h)) \right)
\end{equation}
where $\gamma(h)$ is the semivariogram (that depends on parameter vector $\bm{\theta}$). Minimizing Equation$~$\ref{eq:svcl} yields $\bm{\hat{\theta}}_{cl}$, the semivariogram composite likelihood estimates of $\bm{\theta}$. After estimating $\bm{\theta}$, $\bm{\beta}$ estimates are constructed using (empirical) generalized least squares: $\bm{\hat{\beta}}_{cl} = (\mathbf{X}^\intercal \hat{\mathbf{\Sigma}}^{-1} \mathbf{X})^{-1} \mathbf{X}^\intercal \hat{\mathbf{\Sigma}}^{-1} \mathbf{y}$, where $\hat{\mathbf{\Sigma}}^{-1}$ is $\mathbf{\Sigma}$ evaluated at $\bm{\hat{\theta}}_{cl}$. 

An advantage of the composite likelihood approach to semivariogram estimation is that it does not require arbitrarily specifying empircial semivariogram bins and cutoffs. It does tend to be more computationally demanding than the weighted least squares, however, as the composite likelihood is constructed from $\binom{n}{2}$ pairs for a sample size $n$, whereas the weighted least squares approach only requires calculating $\binom{|N(h)|}{2}$ pairs for each distance bin $N(h)$. As with the weighted least squares approach, Equation$~$\ref{eq:svcl} requires constant-mean process, so typically the residuals from an ordinary least squares regression of $\mathbf{y}$ on $\mathbf{X}$ are used to estimate $\bm{\theta}$.

# Hypothesis Testing

## The General Linear Hypothesis Test

Hypothesis tests for each element in $\hat{\bm{\beta}}$ are available in \texttt{summary()}.
```{r}
spmod <- splm(y ~ x + group, exdata, "exponential", xcoord, ycoord)
summary(spmod)
```

Test statistics are the ratio of the estimate to its standard error. These tests are useful for continuous predictors, but for categorical predictors, \texttt{summary()} only provides test statistics for each level, not all levels simultaneously. To find a test statistic for a categorical variable, there are two approaches. The first is to use \texttt{anova()}. 
```{r}
anova(spmod)
```

Test statistics from \texttt{anova()} are formed using the general linear hypothesis test. Let $\mathbf{L}$ be an $l \times p$ contrast matrix and $l_0$ be an $l \times 1$ vector. The null hypothesis is that $\mathbf{L} \bm{\hat{\beta}} = l_0$ and the alternative hypothesis is that $\mathbf{L} \bm{\hat{\beta}} \neq l_0$. Usually, $l_0$ is the zero vector (and in spmodel, this is assumed). The test statistic, denoted $X^2$, is given by
\begin{equation}\label{eq:glht}
  \tilde{F} = [(\mathbf{L} \bm{\hat{\beta}} - l_0)^\intercal(\mathbf{L} (\mathbf{X}^\intercal \mathbf{\hat{\Sigma}} \mathbf{X})^{-1} \mathbf{L}^\intercal)^{-1}(\mathbf{L} \bm{\hat{\beta}} - l_0)]/ rank(\mathbf{L})
\end{equation}
By default, $\mathbf{L}$ is chosen such that each variable in the data used to fit the model is tested. If this deafult is not desired, the \texttt{Terms} and \texttt{L} arguments can be used to pass user-defined $\mathbf{L}$ matrices to \texttt{anova()}. 

It is notoriously difficult to determine appropriate p-values for linear mixed models based on the general linear hypothesis test. lme4, for example, does not report p-values by default. There are three reasons we focus on next that explain why obtaining p-values is so challenging. 

  * The first (and often most important) challenge is that when estimating $\bm{\theta}$, it is usually not clear what the null distribution of $\tilde{F}$. In certain cases such as ordinary least squares regression or certain experimental designs (e.g., blocked design, split plot design, etc.), $\tilde{F}$ is F-distributed with known numerator and denominator degrees of freedom. But outside of these well-studied cases, no general results exist. 
  * The second challenge is that the standard error of $\tilde{F}$ does not account for the uncertainty in $\bm{\hat{\theta}}$. For some approaches to addressing this problem, see @kackar1984approximations, @prasad1990estimation, @harville1992mean, and @kenward1997small.
  * The third challenge is in determining denominator degrees of freedom. Again, in certain cases, these are known -- but this is not true in general. For some approaches to addressing this problem, see @satterthwaite1946approximate, @schluchter1990small, @hrong1996approximate, @kenward1997small, @littell2006sas, @pinheiro2006mixed, and @kenward2009improved.

For these reasons, spmodel assumes a large-sample, Chi-squared approximation when calculating p-values. This approach addresses the three points above by assuming that with a large enough sample size:

  * The numerator of $\tilde{F}$ tends to be asymptotically Chi-squared (under certain conditions) with $rank(\mathbf{L})$ degrees of freedom.
  * The uncertainty from estimating $\bm{\hat{\theta}}$ is small enough to be safely ignored.

Because the approximation is asymptotic, degree of freedom adjustments can be ignored (it is also worth noting that an F distribution with infinite denominator degrees of freedom is a scaled (by $rank{L}$) Chi-squared distribution). A takeaway here is that this asymptotic approximation implies these p-values are likely unreliable with small samples. 

A second approach to determining p-values is a likelihood ratio test for nested models. Let $l(\bm{\hat{\theta}}_0, \bm{\hat{\beta}}_0 | \mathbf{y} )$ be the log-likelihood from some reduced model and $l(\bm{\hat{\theta}}_1, \bm{\hat{\beta}}_1 | \mathbf{y} )$ be the log-likelihood from some full model. When the reduced model is nested in the full model (i.e., the reduced model can be obtained by fixing some parameters of the full model), $[-2l(\bm{\hat{\theta}}_0, \bm{\hat{\beta}}_0 | \mathbf{y} )] - [-2l(\bm{\hat{\theta}}_1, \bm{\hat{\beta}}_1 | \mathbf{y} )]$ is asymptotically Chi-squared with degrees of freedom equal to the difference in estimated parameters between the full and reduced model. To see whether there is evidence of spatial covariance, run
```{r}
spmod_r <- splm(y ~ x, exdata, "none")
spmod_f <- splm(y ~ x, exdata, "exponential", xcoord, ycoord)
anova(spmod_r, spmod_f)
```
This output suggests evidence of spatial covariance. Because the likelihood relies on minimized likelihoods, they are only defined for ML or REML estimation. Furthermore, for REML estimation, likelihood ratio tests can only be used to compare nested models whose fixed effect structure does not change. This is because the REML likelihood \eqref{eq:reml-lik} depends on the fixed effects through $\ln{|\mathbf{X}^\intercal \mathbf{\Sigma}^{-1} \mathbf{X}|}$. To use likelihood ratio tests for assessing the importance of fixed effects, parameters must be estimated using ML. When using likelihood ratio tests to assess the importance of parameters on the boundary of a parameter space (e.g., a variance parameter being zero), p-values tend to be too large [@self1987asymptotic; @stram1994variance; @goldman2000statistical; @pinheiro2006mixed].

# Fitted Values

Fitted values for the response, spatial random errors, and random effects are obtained in spmodel using `fitted()`. The fitted values for the response, denoted $\mathbf{\hat{y}}$, are given by
\begin{equation}\label{eq:fit_resp}
  \mathbf{\hat{y}} = \mathbf{X} \bm{\hat{\beta}} .
\end{equation}
They are the estimated mean response given the set of covariates for each observation. 

Fitted values for spatial random errors and random effects are linked with best linear unbiased predictors from mixed model theory. Consider the standard random effects parameterization
\begin{equation}
  \mathbf{y} = \mathbf{X} \bm{\beta} + \mathbf{Z} \mathbf{u} + \bm{\epsilon},
\end{equation}
where $\mathbf{Z}$ denotes the random effects design matrix, $\mathbf{u}$ denotes the random effects, and $\bm{\epsilon}$ denotes independent random error. @henderson1975best states that the best linear unbiased predictor (BLUP) of a single random effect $\mathbf{u}$, denoted $\mathbf{\hat{u}}$, is given by
\begin{equation}\label{eq:blup_mm}
  \mathbf{\hat{u}} = \sigma^2_u \mathbf{Z} \mathbf{\Sigma}^{-1}(\mathbf{y} - \mathbf{X} \bm{\hat{\beta}}).
\end{equation}
@searle2009variance generalizes this idea by showing that for a random variable $\bm{\alpha}$, the best linear unbiased predictor (based on the response, $\mathbf{y}$) of $\mathbf{\alpha}$, denoted $\bm{\hat{\alpha}}$ is given by
\begin{equation}\label{eq:blup_gen}
  \bm{\hat{\alpha}} = \text{E}(\bm{\alpha}) + \Sigma_\alpha \Sigma^{-1}(\mathbf{y} - \mathbf{X} \bm{\hat{\beta}}),
\end{equation}
where $\Sigma_\alpha = \text{Cov}(\bm{\alpha}, \mathbf{y})$. Evaluating Equation$~$\eqref{eq:blup_gen} at the plug-in (empirical) estimates of the covariance parameters yields the empirical best linear unbiased predictor (EBLUP) of $\bm{\alpha}$.

Building from this idea and putting plug-in (empirical) estimates of the covariance parameters into Equation$~$\eqref{eq:blup_gen} yields predictors for each random error, which we call fitted values henceforth. For example, the fitted value corresponding to the spatial dependent random error, denoted $\bm{\hat{\tau}}$, is given by
\begin{equation}\label{eq:blup_sp}
  \bm{\hat{\tau}} = \mathbf{\Sigma}_{de} \mathbf{\Sigma}^{-1}(\mathbf{y} - \mathbf{X} \bm{\hat{\beta}}),
\end{equation}
where $\mathbf{\Sigma}_{de} = \text{Cov}(\bm{\tau}, \mathbf{y})$. To obtain these fitted values for the spatial random errors, run `fitted(object, type = "spcov")`. To obtain these fitted values for the random effects, run `fitted(object, type = "randcov")`. 

(Talk about assumption of independent among the random effects yields the EBLUPs for us?)

# Prediction

Predictions are often the primary goal of a data analysis. spmodel performs best linear unbiased prediction, which is equivalent to Kriging, using the \texttt{predict()} function. Let $\mathbf{y}_o$ be a vector of observed variables and $\mathbf{y}_u$ be a vector of unobserved variables. Let $\mathbf{X}_o$ and $\mathbf{X}_u$ be the fixed effect design matrices of $\mathbf{y}_o$ and $\mathbf{y}_u$, respectively. Let $\mathbf{\Sigma}_o$ and $\mathbf{\Sigma}_u$ be the covariance matrices of $\mathbf{y}_o$ and $\mathbf{y}_u$, respectively. The (empirical) best linear unbiased predictor (BLUP; Kriging predictor) of $\mathbf{y}_u$, denoted $\mathbf{\dot{y}_u}$ is given by
\begin{equation}\label{eq:blup}
  \mathbf{\dot{y}_u} =  \mathbf{X}_u \bm{\hat{\beta}} + \mathbf{\hat{c}}_{uo} \mathbf{\hat{\Sigma}}^{-1}_o (\mathbf{y}_o - \mathbf{X}_o \bm{\hat{\beta}}) ,
\end{equation}
where $\mathbf{\hat{c}}_{uo} = \hat{\text{Cov}}(\mathbf{y}_u, \mathbf{y}_o)$ and $\bm{\hat{\beta}} = (\mathbf{X}^\intercal \mathbf{\hat{\Sigma}}^{-1} \mathbf{X})^{-1}$. One can show that $\hat{\text{Cov}}(\mathbf{\dot{y}})$ is
\begin{equation}\label{eq:vblup}
  \text{Cov}(\mathbf{\dot{y}}) = \mathbf{\hat{\Sigma}}_u - \mathbf{\hat{c}}_{uo} \mathbf{\hat{\Sigma}}^{-1}_o \mathbf{\hat{c}}_{uo}^\intercal + \mathbf{H}(\mathbf{X}^\intercal \mathbf{\hat{\Sigma}}^{-1} \mathbf{X})^{-1}\mathbf{H}^\intercal ,
\end{equation}
where $\mathbf{H} = (\mathbf{X}_u - \mathbf{\hat{c}}_{uo} \mathbf{\hat{\Sigma}}^{-1}_o \mathbf{X}_o)$. When $\texttt{predict(..., interval = "none")}$, predictions are obtained using Equation$~$\ref{eq:blup}. If \texttt{se.fit = TRUE}, standard errors are the square root of the diagonal of Equation$~$\ref{eq:vblup}. When \texttt{predict(..., interval = "prediction")}, predictions are obtained using Equation$~$\ref{eq:blup} and standard errors used for the lower and upper bound of the prediction intervals are obtained via the square root of the diagonal of Equation$~$\ref{eq:vblup} (these are also what are returned if \texttt{se.fit = TRUE}). When \texttt{predict(..., interval = "confidence")}, mean estimates are obtained using $\mathbf{X}_u \bm{\hat{\beta}}$ and standard errors used for the lower and upper bound of the confidence intervals are obtained via the square root of the diagonal of $(\mathbf{X}^\intercal \mathbf{\hat{\Sigma}}^{-1} \mathbf{X})^{-1}$ (these are also what are returned if \texttt{se.fit = TRUE}). 

For autoregressive models, an extra step is required to obtain $\hat{\Sigma}^{-1}_o$ and $\hat{\Sigma}^{-1}_u$, as they depend on one another through the locations of $\mathbf{y}_o$ and $\mathbf{y}_u$. Let $\hat{\Sigma}^{-1}$ be the inverse covariance matrix of $\mathbf{y}_o$ and $\mathbf{y}_u$ (which we directly specify for autoregressive models). If the data are small enough, then $\hat{\Sigma}^{-1}$ can be inverted, yielding $\hat{\Sigma}$. Then appropriate subsets can then used to obtain the components needed for prediction. A faster way to obtain relevant components required for prediction does exist and is worth mentioning -- $\hat{\Sigma}^{-1}$ can be represented blockwise as
\begin{equation}\label{eq:auto_hw}
  \hat{\Sigma}^{-1} =
  \begin{bmatrix}
    \mathbf{\hat{\Sigma}}^{*}_{o, o} & \mathbf{\hat{\Sigma}}^{*}_{o, u} \\
    \mathbf{\hat{\Sigma}}^{*}_{u, o} & \mathbf{\hat{\Sigma}}^{*}_{u, u}
  \end{bmatrix},
\end{equation}
where the dimensions of the blocks are indexed by the matrix subscripts (rows then columns). All of the terms required for prediction can be obtained from this block representation. @wolf1978helmert shows that 
\begin{equation}\label{eq:hw_forms}
  \begin{split}
    \hat{\Sigma}^{-1}_o & = \mathbf{\hat{\Sigma}}^{*}_{o, o} - \mathbf{\hat{\Sigma}}^{*}_{o, u} (\mathbf{\hat{\Sigma}}^{*}_{u, u})^{-1} \mathbf{\hat{\Sigma}}^{*}_{u, o} \\
    \hat{\Sigma}_u & = (\mathbf{\hat{\Sigma}}^{*}_{u, u} - \mathbf{\hat{\Sigma}}^{*}_{u, o} (\mathbf{\hat{\Sigma}}^{*}_{o, o})^{-1} \mathbf{\hat{\Sigma}}^{*}_{o, u})^{-1} \\
    \text{Cov}(\mathbf{y}_o, \mathbf{y}_u) & = - \hat{\Sigma}_o \mathbf{\hat{\Sigma}}^{*}_{o, u} (\mathbf{\hat{\Sigma}}^{*}_{u, u})^{-1}
  \end{split}
\end{equation}
A similar result exists for the log determinant (used in ML and REML estimation).

# (Geometric) Anisotropy

A spatial process is geometrically isotropic if its covariance decays equally in all directions. A spatial process is geometrically anisotropic if its covariance does not decay equally in all directions (i.e., is not isotropic). Let $h$ represent distance, $\alpha$ represent an angle in $[0, \pi]$, and $\tau$ represent a scaling factor in $[0, 1]$. Geometric anisotropy is accommodate through a specific rotation and scaling of the x-coordinate and y-coordinate. Then a new distance, $h*$, corresponds to an isotropic process. The scaling works by rotating the major axis of the covariance clockwise onto a transformed x-axis and scaling the minor axis of the covariance along the transformed y-axis. Geometrically, this transformation and re-scaling is achieved via
\begin{equation}
  \begin{bmatrix}
    \mathbf{x}^* \\
    \mathbf{y}^*
  \end{bmatrix} = 
  \begin{bmatrix}
    1 & 0 \\
    0 & 1 / \tau
  \end{bmatrix}
  \begin{bmatrix}
    \cos(\alpha) & \sin(\alpha) \\
    -\sin(\alpha) & \cos(\alpha)
  \end{bmatrix}  
  \begin{bmatrix}
    \mathbf{x} \\
    \mathbf{y}
  \end{bmatrix}
\end{equation}

Then covariance parameters are estimated using the transformed coordinates to compute distances. An anisotropy transformation is visualized in Figure$~$\ref{fig:anisotropy}.

```{r anisotropy, out.width = "33%", fig.show = "hold", fig.cap = "In the first figure, the ellipse of an anisotropy covariance is shown. In the second figure, the ellipse is rotated so the major axis is the transformed x-axis and the minor axis is the transformed y-axis. In the third figure, the ellipse is scaled so the ellipse becomes a circle. Then covariance estimation can be performed (using the transformed coordinates that compose the third figure)."}
# PRELIMINARIES
r <- 1
theta_seq <- seq(0, 2 * pi, length.out = 1000)
x_orig <- r * cos(theta_seq)
y_orig <- r * sin(theta_seq)
df_orig <- data.frame(x = x_orig, y = y_orig)

# FIRST FIGURE
theta <- pi / 4 # (45 degrees)
R <- 1 / 4 # (minor axis length / major axis length)
scale <- matrix(c(1, 0, 0, R), nrow = 2, byrow = TRUE)
rotate <- matrix(c(cos(theta), -sin(theta), sin(theta), cos(theta)), nrow = 2, byrow = TRUE)
transform <- rotate %*% scale
coords <- transform %*% rbind(df_orig[["x"]], df_orig[["y"]])
df <- data.frame(x = coords[1, ], y = coords[2, ])
ggplot(df, aes(x = x, y = y)) + 
  geom_point() + 
  lims(x = c(-1, 1), y = c(-1, 1))  +
  geom_hline(yintercept = 0, col = "blue") + 
  geom_vline(xintercept = 0, col = "blue", lty = "dashed") + 
  geom_abline(intercept = 0, slope = 1, col = "red") +
  geom_abline(intercept = 0, slope = -1, col = "red", lty = "dashed")

# SECOND FIGURE
rotate_anis <- matrix(c(cos(theta), sin(theta), -sin(theta), cos(theta)), nrow = 2, byrow = TRUE)
coords_anis <- rotate_anis %*% rbind(df[["x"]], df[["y"]])
df_anis <- data.frame(x = coords_anis[1, ], y = coords_anis[2, ])
ggplot(df_anis, aes(x = x, y = y)) + 
  geom_point() +
  lims(x = c(-1, 1), y = c(-1, 1))  +
  geom_hline(yintercept = 0, col = "red") + 
  geom_vline(xintercept = 0, col = "red", lty = "dashed") + 
  geom_abline(intercept = 0, slope = 1, col = "blue", lty = "dashed") +
  geom_abline(intercept = 0, slope = -1, col = "blue")

# THIRD FIGURE
unscale <- matrix(c(1, 0, 0, 1 / R), nrow = 2, byrow = TRUE)
coords_iso <- unscale %*% rbind(df_anis[["x"]], df_anis[["y"]])
df_iso <- data.frame(x = coords_iso[1, ], y = coords_iso[2, ])
ggplot(df_iso, aes(x = x, y = y)) + 
  geom_point() +
  lims(x = c(-1, 1), y = c(-1, 1))  +
  geom_hline(yintercept = 0, col = "red") + 
  geom_vline(xintercept = 0, col = "red", lty = "dashed") + 
  geom_abline(intercept = 0, slope = 1, col = "blue", lty = "dashed") +
  geom_abline(intercept = 0, slope = -1, col = "blue")
```


In spmodel, anisotropy parameters ($\alpha$ and $\tau$) can be estimated using ML or REML by setting the \texttt{anisotropy} argument to \texttt{TRUE} or by specifying one of the anisotropy parameters as unknown or known and different from zero in \texttt{spcov\_initial}. Fixed anisotropy parameters can be provided to SV-WLS and SV-CL and incorporated into estimation of $\bm{\theta}$ and $\bm{\beta}$ when specified in \texttt{spcov\_initial}.

Estimating anisotropy can be challenging because of local minima in the likelihood. First note that estimation need only occur for $\alpha$ on $(0, \pi)$ radians due to symmetric of the covariance ellipse at rotations $\alpha$ and $\alpha + \pi$. Second note that estimation need only occur for $\tau$ on $(0, 1)$ because we have defined $\tau$ as the scaling factor for the length of the minor axis relative to the major axis. To address the local minima problem, each optimization iteration actually involves two likelihood evaluations, one for $\alpha = \alpha_0$ and another for $pi - \alpha_0$, so that the likelhood is evaluated for $\alpha$ parameters the quadrants on either side of the minor (transformed y) axis.

# The Local list

# References {.unnumbered}
