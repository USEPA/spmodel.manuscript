\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{lmodern}        % https://github.com/rstudio/rticles/issues/343
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}

\title{spmodel: Spatial Modeling in \textbf{R} -- Computatinal Details Vignette}

\author{
    Michael Dumelle
    \thanks{Corresponding Author}
   \\
    United States \\
    Environmental Protection Agency \\
  200 SW 35th St, Corvallis, OR, 97333 \\
  \texttt{\href{mailto:Dumelle.Michael@epa.gov}{\nolinkurl{Dumelle.Michael@epa.gov}}} \\
   \And
    Matt Higham
   \\
    Department of Math, Computer Science, and Statistics \\
    St.~Lawrence University \\
  23 Romoda Drive, Canton, NY, 13617 \\
  \texttt{\href{mailto:mhigham@stlawu.edu}{\nolinkurl{mhigham@stlawu.edu}}} \\
   \And
    Jay M. Ver Hoef
   \\
    National Oceanic and Atmospheric Administration \\
    Alaska Fisheries Science Center \\
  Marine Mammal Laboratory, Seattle, WA, 98115 \\
  \texttt{\href{mailto:jay.verhoef@noaa.gov}{\nolinkurl{jay.verhoef@noaa.gov}}} \\
  }


% tightlist command for lists without linebreak
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



\usepackage{amsmath,amsfonts,amssymb}
\usepackage{bm, bbm}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs=true}
\begin{document}
\maketitle


\begin{abstract}
Fit, summarize, and predict for a variety of spatial models. Parameters
of spatial linear models and spatial autoregressive models are estimated
using a variety of methods. Additional modeling features include
anisotropy, random effects, partition factors, big data approaches, and
more. Model-fit statistics are used to summarize, visualize, and compare
models. Predictions at unobserved locations are easily obtainable.
\end{abstract}

\keywords{
    Spatial covariance
   \and
    Linear Model
   \and
    Autoregressive model
  }

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

This vignette covers technical details regarding the functions in
spmodel that perform computations.

A note on square roots and Cholesky solves

\hypertarget{splm-and-spautor}{%
\section{\texorpdfstring{\texttt{splm()} and
\texttt{spautor()}}{splm() and spautor()}}\label{splm-and-spautor}}

\hypertarget{splm}{%
\subsection{`splm()}\label{splm}}

\hypertarget{additional-functions}{%
\section{Additional Functions`}\label{additional-functions}}

\hypertarget{covariance-functions}{%
\subsubsection{Covariance Functions}\label{covariance-functions}}

\hypertarget{estimation}{%
\subsubsection{Estimation}\label{estimation}}

\hypertarget{likelihood-based-estimation}{%
\paragraph{Likelihood-based
Estimation}\label{likelihood-based-estimation}}

Minus twice a profiled Gaussian log-likelihood, denoted
\(-2l(\bm{\theta} | \mathbf{y})\) is given by
\begin{equation}\label{eq:ml-lik}
  -2l(\bm{\theta} | \mathbf{y}) = \ln{|\mathbf{\Sigma}|} + (\mathbf{y} - \mathbf{X} \tilde{\bm{\beta}})^\intercal \mathbf{\Sigma}^{-1} (\mathbf{y} - \mathbf{X} \tilde{\bm{\beta}}) + n \ln{2\pi},
\end{equation} where
\(\tilde{\bm{\beta}} = (\mathbf{X}^\intercal \mathbf{\Sigma}^{-1} \mathbf{X})^{-1} \mathbf{X}^\intercal \mathbf{\Sigma}^{-1} \mathbf{y}\).
Minimizing Equation\(~\)\ref{eq:ml-lik} yields
\(\bm{\hat{\theta}}_{ml}\), the maximum likelihood estimates for
\(\bm{\theta}\). Then a closed for solution exists for
\(\bm{\hat{\beta}}_{ml}\), the maximum likelihood estimates for
\(\bm{\beta}\): \(\bm{\hat{\beta}}_{ml} = \tilde{\bm{\beta}}_{ml}\),
where \(\tilde{\bm{\beta}}_{ml}\) is \(\tilde{\bm{\beta}}\) evaluated at
\(\bm{\hat{\theta}}_{ml}\). Unfortunately \(\bm{\hat{\theta}}_{ml}\) can
be badly biased for \(\bm{\theta}\) (especially for small sample sizes),
which impacts the estimation of \(\bm{\beta}\) (Patterson and Thompson
1971). This bias occurs due to the simultaneous estimation of
\(\bm{\beta}\) and \(\bm{\theta}\) To reduce this bias, restricted
maximum likelihood estimation (REML) emerged (Patterson and Thompson
1971; Harville 1977; Wolfinger, Tobias, and Sall 1994). It can be shown
that integrating \(\bm{\beta}\) out of a Gaussian likelihood yields the
restricted Gaussian likelihood used in REML estimation. Minus twice a
restricted Gaussian log-likelihood, denoted
\(-2l_R(\bm{\theta} | \mathbf{y})\) is given by
\begin{equation}\label{eq:reml-lik}
  -2l_R(\bm{\theta} | \mathbf{y}) = -2l(\bm{\theta} | \mathbf{y})  + \ln{|\mathbf{X}^\intercal \mathbf{\Sigma}^{-1} \mathbf{X}|} - p \ln{2\pi} ,
\end{equation} where \(p\) equals the dimension of \(\bm{\beta}\).
Minimizing Equation\(~\)\ref{eq:reml-lik} yields
\(\bm{\hat{\theta}}_{reml}\), the restricted maximum likelihood
estimates for \(\bm{\theta}\). Then a closed for solution exists for
\(\bm{\hat{\beta}}_{reml}\), the restricted maximum likelihood estimates
for \(\bm{\beta}\):
\(\bm{\hat{\beta}}_{reml} = \tilde{\bm{\beta}}_{reml}\), where
\(\tilde{\bm{\beta}}_{reml}\) is \(\tilde{\bm{\beta}}\) evaluated at
\(\bm{\hat{\theta}}_{reml}\).

Generally the overall variance, \(\sigma^2\), can be profiled out of
Equation\(~\)\ref{eq:ml-lik} and Equation\(~\)\ref{eq:reml-lik}. This
reduces the number of parameters requiring optimization by one, which
can dramatically reduce estimation time. For example, profiling
\(\sigma^2\) out of Equation\(~\)\ref{eq:ml-lik} yields
\begin{equation}\label{eq:ml-plik}
  -2l^*(\bm{\theta}^* | \mathbf{y}) = \ln{|\mathbf{\Sigma^*}|} + n\ln[(\mathbf{y} - \mathbf{X} \tilde{\bm{\beta}})^\intercal \mathbf{\Sigma}^{-1} (\mathbf{y} - \mathbf{X} \tilde{\bm{\beta}})] + n + n\ln{2\pi / n}.
\end{equation} After finding \(\hat{\bm{\theta}}^*_{ml}\) a closed form
solution for \(\hat{\sigma}^2_{ml}\) exists:
\(\hat{\sigma}^2_{ml} = [(\mathbf{y} - \mathbf{X} \bm{\tilde{\beta}})^\intercal \mathbf{\Sigma}^{-1} (\mathbf{y} - \mathbf{X} \tilde{\bm{\beta}})] / n\).
Then \(\bm{\hat{\theta}}^*_{ml}\) is combined with
\(\hat{\sigma}^2_{ml}\) to yield \(\bm{\hat{\theta}}_{ml}\) and
subsequently \(\bm{\hat{\beta}}_{ml}\). A similar result holds for REML
estimation. Profiling \(\sigma^2\) out of Equation\(~\)\ref{eq:reml-lik}
yields \begin{equation}\label{eq:reml-plik}
  -2l_R^*(\bm{\theta}^* | \mathbf{y}) = \ln{|\mathbf{\Sigma^*}|} + (n - p)\ln[(\mathbf{y} - \mathbf{X} \tilde{\bm{\beta}})^\intercal \mathbf{\Sigma}^{-1} (\mathbf{y} - \mathbf{X} \tilde{\bm{\beta}})] + (n - p) + (n - p)\ln2\pi / (n - p).
\end{equation} After finding \(\hat{\bm{\theta}}^*_{reml}\) a closed
form solution for \(\hat{\sigma}^2_{reml}\) exists:
\(\hat{\sigma}^2_{reml} = [(\mathbf{y} - \mathbf{X} \bm{\tilde{\beta}})^\intercal \mathbf{\Sigma}^{-1} (\mathbf{y} - \mathbf{X} \tilde{\bm{\beta}})] / (n - p)\).
Then \(\bm{\hat{\theta}}^*_{reml}\) is combined with
\(\hat{\sigma}^2_{reml}\) to yield \(\bm{\hat{\theta}}_{reml}\) and
subsequently \(\bm{\hat{\beta}}_{reml}\).

Both ML and REML estimation rely on the an \(n \times n\) covariance
matrix inverse. Inverting an \(n \times n\) matrix is an enormous
computational demand that scales cubically with the sample size. For
this reason, ML and REML have historically been unfeasible to implement
in their standard form with data larger than a few thousand
observations. This motivates the use for the big data approaches
outlined in Section\(~\)(INSERT SECTION).

It is worth noting that the inverses themselves are not strictly needed
for estimation (or prediction), though at least their square root is
needed. In spmodel, calculating this square root requires a Cholesky
decomposition, which still scales cubically with the sample size.
Computing the Cholesky decomposition, however, is far more
computationally efficient than computing the inverse. To see why only
the Cholesky decomposition is needed, recall that the Cholesky
decomposition of the covariance matrix \(\mathbf{\Sigma}\) is
\(\mathbf{C}\mathbf{C}^\intercal\), where \(\mathbf{C}\) is a lower
triangular matrix (so
\(\mathbf{C}\mathbf{C}^\intercal = \mathbf{\Sigma}\)). In the ML and
REML likelihoods, \(\mathbf{\Sigma}^{-1}\) is not needed on its own,
only \(\mathbf{X}^\intercal \mathbf{\Sigma}^{-1} \mathbf{X}\) and
\(\mathbf{X}^\intercal \mathbf{\Sigma}^{-1} \mathbf{y}\) are needed. We
can rewrite the \(\mathbf{X}^\intercal \mathbf{\Sigma}^{-1} \mathbf{X}\)
as
\(\mathbf{X}^\intercal (\mathbf{C}^\intercal)^{-1} \mathbf{C}^{-1} \mathbf{X} = (\mathbf{C}^{-1} \mathbf{X})^\intercal \mathbf{C}^{-1} \mathbf{X}\).
Then \(\mathbf{C}^{-1} \mathbf{X}\) is efficiently solved by noticing
that \(\mathbf{C}^{-1} \mathbf{X} = \mathbf{A}\) for some matrix
\(\mathbf{A}\) implies \(\mathbf{X} = \mathbf{C} \mathbf{A}\). This
system can be efficiently solved for \(\mathbf{A}\) using linear forward
solves (forward substitution). Then
\(\mathbf{X}^\intercal \mathbf{\Sigma}^{-1} \mathbf{X} = \mathbf{A}^\intercal \mathbf{A}\).
A similar approach is used to solve
\(\mathbf{X}^\intercal \mathbf{\Sigma}^{-1} \mathbf{y}\). Still, using
Cholesky decomositions is unfeasible for sample sizes larger than a few
thousand observations.

\hypertarget{semivariogram-based-estimation}{%
\paragraph{Semivariogram-based
Estimation}\label{semivariogram-based-estimation}}

An alternative approach to likelihood-based estimation is
semivariogram-based estimation. The semivariogram of a constant-mean
process \(\mathbf{y}\) is the expectation of the squared half-difference
between two observations \(h\) distance units apart. More formally, the
semivariogram is denoted \(\gamma(h)\) and defined as
\begin{equation}\label{eq:sv}
  \gamma(h) = \text{E}(y_i - y_j)^2 / 2 ,
\end{equation} where \(||y_i - y_j||_2 = h\) (the Euclidean distance).
When the process \(\mathbf{y}\) is second-order stationary, the
semivariogram and covariance function are intimately connected:
\(\gamma(h) = \text{Cov}(0) - \text{Cov}(h)\), where \(\text{Cov}(0)\)
is the covariance function evaluated at 0 (which is the overall
variance, \(\sigma^2\)) and \(\text{Cov}(h)\)is the covariance function
evaluated at \(h\). Both semivariogram approaches described next are
more computationally efficient than ML or REML because their major
computational burden (calculations based on pairs) scale the squared
sample size (i.e., not the cubed sample size).

\hypertarget{weighted-least-squares}{%
\subparagraph{Weighted Least Squares}\label{weighted-least-squares}}

The empirical semivariogram is a moment-based estimate of the
semivariogram denoted by \(\hat{\gamma}(h)\) and defined as
\begin{equation}\label{eq:esv}
  \hat{\gamma}(h) = \frac{1}{2|N(h)|} \sum_{N(h)} (y_i - y_j)^2, 
\end{equation} where \(N(h)\) is the set of observations in
\(\mathbf{y}\) that are \(h\) units apart (distance classes) and
\(|N(h)|\) is the cardinality of \(N(h)\) (Cressie 1993). Often the set
\(N(h)\) contains observations that are \(h \pm \alpha\) apart -- this
approach is known as ``binning'' the empirical semivariogram. Typically,
only certain \(h\) considered when constructing
Equation\(~\)\ref{eq:esv} -- a commonly used cutoff is to ignore \(h\)
larger than half the maximum distance in the domain. One criticism of
the empirical semivariogram is that distance bins and cutoffs tend to be
artitrarily chosen (i.e., not chosen according to some statistical
criteria).

Equation\(~\)\eqref{eq:esv} is viewed as the average squared
half-distance between two observations in \(\mathbf{y}\). Cressie (1985)
proposed estimating \(\bm{\theta}\) by minimizing an objective function
that involves \(\gamma{h}\) and \(\hat{\gamma}(h)\) and is based on a
weighted least squares criterion. This criterion is defined as
\begin{equation}\label{eq:svwls}
  \sum_i w_i [\hat{\gamma}(h)_i - \gamma(h)_i]^2,
\end{equation} where \(w_i\), \(\hat{\gamma}(h)_i\), and \(\gamma(h)_i\)
are the weights, empirical semivariogram, and semivariogram for the
\(i\)th distance class. Minimizing Equation\(~\)\eqref{eq:svwls} yields
\(\bm{\hat{\theta}}_{wls}\), the semivariogram weighted least squares
estimates of \(\bm{\theta}\). After estimating \(\bm{\theta}\),
\(\bm{\beta}\) estimates are constructed using (empirical) generalized
least squares:
\(\bm{\hat{\beta}}_{wls} = (\mathbf{X}^\intercal \hat{\mathbf{\Sigma}}^{-1} \mathbf{X})^{-1} \mathbf{X}^\intercal \hat{\mathbf{\Sigma}}^{-1} \mathbf{y}\),
where \(\hat{\mathbf{\Sigma}}^{-1}\) is \(\mathbf{\Sigma}\) evaluated at
\(\bm{\hat{\theta}}_{wls}\).

Cressie (1985) recommends setting the \(w_i\) in
Equation\(~\)\eqref{eq:svwls} as \(w_i = |N(h)| / \gamma(h)_i^2\), which
gives more weights to distance classes with more observations
(\(|N(h)|\)) and semivariances at shorter distances
(\(1 / \gamma(h)_i^2\)). The default in spmodel is to use these \(w_i\)
-- the type of \(w_i\) is changed via the \texttt{weights} argument to
\texttt{splm()}. Table\(~\)\ref{tab:weights} contains all \(w_i\)
available in spmodel.

\begin{table}\label{tab:weights}
  \centering
  \begin{tabular}{c|c|c}
  \hline
  $w_i$ Name & $w_i$ Form & \texttt{weight = } \\
  Cressie & $|N(h)| / \gamma(h)_i^2$ & \texttt{"cressie"} \\
  Cressie (Denominator) Root & $|N(h)| / \gamma(h)_i$ & \texttt{"cressie-droot"} \\
  Cressie No Pairs & $1 / \gamma(h)_i^2$ & \texttt{"cressie-nopairs"} \\
  Cressie (Denominator) Root No Pairs & $1 / \gamma(h)_i$ & \texttt{"cressie-droot-nopairs"} \\
  Pairs & $|N(h)|$ & \texttt{"pairs"} \\
  Pairs Inverse Distance & $|N(h)| / h^2$ & \texttt{"pairs-invd"} \\
  Pairs Inverse (Root) Distance & $|N(h)| / h$ & \texttt{"pairs-invsd"} \\
  Ordinary Least Squares & 1 & \texttt{ols}
  \end{tabular}
  \caption{spmodel table weights}
\end{table}

Additionally, the number of \(N(h)\) classes and maximum distance for
\(h\) are specified by passing the \texttt{bins} and \texttt{cutoff}
arguments to \texttt{splm()} (these arguments are passed via
\texttt{...} to \texttt{esv()}). The default value for \texttt{bins} is
15 and the default value for the maximum \(h\) is half the maximum
distance of the spatial domain's bounding box.

Recall that the semivariogram is defined for a constant-mean process.
Generally, \(\mathbf{y}\) does not necessarily have a constant mean. So
the empirical semivariogram and \(\bm{\hat{\theta}}_{wls}\) are
typically constructed using the residuals from an ordinary least squares
regression of \(\mathbf{y}\) on \(\mathbf{X}\) -- these residuals are
assumed to have mean zero.

\hypertarget{composite-likelihood}{%
\subparagraph{Composite Likelihood}\label{composite-likelihood}}

The composite likelihood approach involves constructing a likelihood
based on conditional or marginal events for which log-likelihoods are
available and then adding together these individual components.
Composite likelihoods are attractive because they behave very similar to
likelihoods but are easier to handle, both from a theoretical and a
computational perspective. Curriero and Lele (1999) derive a particular
composite likelihood for estimating semivariogram parameters. The
negative log of this composite likelihood, denoted \(\text{CL}(h)\), is
given by \begin{equation}\label{eq:svcl}
  \text{CL}(h) = \sum_{i = 1}^{n - 1} \sum_{j > i} \left( \frac{(y_i - y_j)^2}{2\gamma(h)} + \ln(\gamma(h)) \right)
\end{equation} where \(\gamma(h)\) is the semivariogram (that depends on
parameter vector \(\bm{\theta}\)). Minimizing Equation\(~\)\ref{eq:svcl}
yields \(\bm{\hat{\theta}}_{cl}\), the semivariogram composite
likelihood estimates of \(\bm{\theta}\). After estimating
\(\bm{\theta}\), \(\bm{\beta}\) estimates are constructed using
(empirical) generalized least squares:
\(\bm{\hat{\beta}}_{cl} = (\mathbf{X}^\intercal \hat{\mathbf{\Sigma}}^{-1} \mathbf{X})^{-1} \mathbf{X}^\intercal \hat{\mathbf{\Sigma}}^{-1} \mathbf{y}\),
where \(\hat{\mathbf{\Sigma}}^{-1}\) is \(\mathbf{\Sigma}\) evaluated at
\(\bm{\hat{\theta}}_{cl}\).

An advantage of the composite likelihood approach to semivariogram
estimation is that it does not require arbitrarily specifying empircial
semivariogram bins and cutoffs. It does tend to be more computationally
demanding than the weighted least squares, however, as the composite
likelihood is constructed from \(\binom{n}{2}\) pairs for a sample size
\(n\), whereas the weighted least squares approach only requires
calculating \(\binom{|N(h)|}{2}\) pairs for each distance bin \(N(h)\).
As with the weighted least squares approach, Equation\(~\)\ref{eq:svcl}
requires constant-mean process, so typically the residuals from an
ordinary least squares regression of \(\mathbf{y}\) on \(\mathbf{X}\)
are used to estimate \(\bm{\theta}\).

\hypertarget{optimization}{%
\subsubsection{Optimization}\label{optimization}}

\hypertarget{hypothesis-testing}{%
\subsubsection{Hypothesis Testing}\label{hypothesis-testing}}

\hypertarget{geometric-anisotropy}{%
\subsubsection{(Geometric) Anisotropy}\label{geometric-anisotropy}}

A spatial process is geometrically isotropic if its covariance decays
equally in all directions. A spatial process is geometrically
anisotropic if its covariance does not decay equally in all directions
(i.e., is not isotropic). Let \(h\) represent distance, \(\alpha\)
represent an angle in \([0, \pi]\), and \(\tau\) represent a scaling
factor in \([0, 1]\). Geometric anisotropy is accommodate through a
specific rotation and scaling of the x-coordinate and y-coordinate. Then
a new distance, \(h*\), corresponds to an isotropic process. The scaling
works by rotating the major axis of the covariance clockwise onto a
transformed x-axis and scaling the minor axis of the covariance along
the transformed y-axis. Geometrically, this transformation and
re-scaling is achieved via \begin{equation}
  \begin{bmatrix}
    \mathbf{x}^* \\
    \mathbf{y}^*
  \end{bmatrix} = 
  \begin{bmatrix}
    1 & 0 \\
    0 & 1 / \tau
  \end{bmatrix}
  \begin{bmatrix}
    \cos(\alpha) & \sin(\alpha) \\
    -\sin(\alpha) & \cos(\alpha)
  \end{bmatrix}  
  \begin{bmatrix}
    \mathbf{x} \\
    \mathbf{y}
  \end{bmatrix}
\end{equation}

Then covariance parameters are estimated using the transformed
coordinates to compute distances. An anisotropy transformation is
visualized in Figure\(~\)\ref{fig:anisotropy}.

\begin{figure}
\includegraphics[width=0.33\linewidth]{supplementary_files/figure-latex/anisotropy-1} \includegraphics[width=0.33\linewidth]{supplementary_files/figure-latex/anisotropy-2} \includegraphics[width=0.33\linewidth]{supplementary_files/figure-latex/anisotropy-3} \caption{In the first figure, the ellipse of an anisotropy covariance is shown. In the second figure, the ellipse is rotated so the major axis is the transformed x-axis and the minor axis is the transformed y-axis. In the third figure, the ellipse is scaled so the ellipse becomes a circle. Then covariance estimation can be performed (using the transformed coordinates that compose the third figure).}\label{fig:anisotropy}
\end{figure}

In spmodel, anisotropy parameters (\(\alpha\) and \(\tau\)) can be
estimated using ML or REML by setting the \texttt{anisotropy} argument
to \texttt{TRUE} or by specifying one of the anisotropy parameters as
unknown or known and different from zero in \texttt{spcov\_initial}.
Fixed anisotropy parameters can be provided to SV-WLS and SV-CL and
incorporated into estimation of \(\bm{\theta}\) and \(\bm{\beta}\) when
specified in \texttt{spcov\_initial}.

Estimating anisotropy can be challenging because of local minima in the
likelihood. First note that estimation need only occur for \(\alpha\) on
\((0, \pi)\) radians due to symmetric of the covariance ellipse at
rotations \(\alpha\) and \(\alpha + \pi\). Second note that estimation
need only occur for \(\tau\) on \((0, 1)\) because we have defined
\(\tau\) as the scaling factor for the length of the minor axis relative
to the major axis. To address the local minima problem, each
optimization iteration actually involves two likelihood evaluations, one
for \(\alpha = \alpha_0\) and another for \(pi - \alpha_0\), so that the
likelhood is evaluated for \(\alpha\) parameters the quadrants on either
side of the minor (transformed y) axis.

\hypertarget{the-local-list}{%
\subsubsection{The Local list}\label{the-local-list}}

\hypertarget{spautor}{%
\subsection{\texorpdfstring{\texttt{spautor()}}{spautor()}}\label{spautor}}

\hypertarget{additional-functions-1}{%
\section{Additional Functions}\label{additional-functions-1}}

\hypertarget{aic-and-aicc}{%
\subsection{\texorpdfstring{\texttt{AIC()} and
\texttt{AICc()}}{AIC() and AICc()}}\label{aic-and-aicc}}

The \texttt{AIC()} and \texttt{AICc()} functions in spmodel follow
Hoeting et al. (2006), who define spatial AIC and AICc as
\begin{equation*}\label{eq:sp_aic}
  \begin{split}
    \text{AIC} & = -2\ln(\hat{L}) + 2(npar + 1) \\
    \text{AICc} & = -2\ln(\hat{L}) + 2n(npar + 1) / (n - npar - 2),
  \end{split}
\end{equation*} where \(\ln(\hat{L})\) is the log likelihood evaluated
at its maximum given the estimated parameters, \(npar\) is the number of
estimated parameters, and \(n\) is the sample size. AIC and AICc are
only defined for restricted maximum likelihood and maximum likelihood
estimation, as they are the only estimation methods in spmodel that
maximize a likelihood. For restricted maximum likelihood, \(npar\) is
the number of estimated covariance parameters. For maximum likelihood,
\(npar\) is the number of estimated covariance parameters plus the
number of estimated fixed effects. The discrepancy arises because
restricted maximum likelihood integrates the fixed effects out of the
likelihood, thus the likelihood does not depend on the fixed effects
(though it does still depend on the predictor variables).

\hypertarget{anova}{%
\subsection{\texorpdfstring{\texttt{anova()}}{anova()}}\label{anova}}

\hypertarget{the-general-linear-hypothesis-test}{%
\subsubsection{The General Linear Hypothesis
Test}\label{the-general-linear-hypothesis-test}}

Test statistics from \texttt{anova()} are formed using the general
linear hypothesis test. Let \(\mathbf{L}\) be an \(l \times p\) contrast
matrix and \(l_0\) be an \(l \times 1\) vector. The null hypothesis is
that \(\mathbf{L} \bm{\hat{\beta}} = l_0\) and the alternative
hypothesis is that \(\mathbf{L} \bm{\hat{\beta}} \neq l_0\). Usually,
\(l_0\) is the zero vector (and in spmodel, this is assumed). The test
statistic, denoted \(X^2\), is given by \begin{equation}\label{eq:glht}
  \tilde{F} = [(\mathbf{L} \bm{\hat{\beta}} - l_0)^\intercal(\mathbf{L} (\mathbf{X}^\intercal \mathbf{\hat{\Sigma}} \mathbf{X})^{-1} \mathbf{L}^\intercal)^{-1}(\mathbf{L} \bm{\hat{\beta}} - l_0)]/ rank(\mathbf{L})
\end{equation} By default, \(\mathbf{L}\) is chosen such that each
variable in the data used to fit the model is tested. If this deafult is
not desired, the \texttt{Terms} and \texttt{L} arguments can be used to
pass user-defined \(\mathbf{L}\) matrices to \texttt{anova()}.

It is notoriously difficult to determine appropriate p-values for linear
mixed models based on the general linear hypothesis test. lme4, for
example, does not report p-values by default. There are three reasons we
focus on next that explain why obtaining p-values is so challenging.

\begin{itemize}
\tightlist
\item
  The first (and often most important) challenge is that when estimating
  \(\bm{\theta}\), it is usually not clear what the null distribution of
  \(\tilde{F}\). In certain cases such as ordinary least squares
  regression or certain experimental designs (e.g., blocked design,
  split plot design, etc.), \(\tilde{F}\) is F-distributed with known
  numerator and denominator degrees of freedom. But outside of these
  well-studied cases, no general results exist.
\item
  The second challenge is that the standard error of \(\tilde{F}\) does
  not account for the uncertainty in \(\bm{\hat{\theta}}\). For some
  approaches to addressing this problem, see Kackar and Harville (1984),
  Prasad and Rao (1990), Harville and Jeske (1992), and Kenward and
  Roger (1997).
\item
  The third challenge is in determining denominator degrees of freedom.
  Again, in certain cases, these are known -- but this is not true in
  general. For some approaches to addressing this problem, see
  Satterthwaite (1946), Schluchter and Elashoff (1990), Hrong-Tai Fai
  and Cornelius (1996), Kenward and Roger (1997), Littell et al. (2006),
  Pinheiro and Bates (2006), and Kenward and Roger (2009).
\end{itemize}

For these reasons, spmodel assumes a large-sample, Chi-squared
approximation when calculating p-values. This approach addresses the
three points above by assuming that with a large enough sample size:

\begin{itemize}
\tightlist
\item
  The numerator of \(\tilde{F}\) tends to be asymptotically Chi-squared
  (under certain conditions) with \(rank(\mathbf{L})\) degrees of
  freedom.
\item
  The uncertainty from estimating \(\bm{\hat{\theta}}\) is small enough
  to be safely ignored.
\end{itemize}

Because the approximation is asymptotic, degree of freedom adjustments
can be ignored (it is also worth noting that an F distribution with
infinite denominator degrees of freedom is a scaled (by \(rank{L}\))
Chi-squared distribution). A takeaway here is that this asymptotic
approximation implies these p-values are likely unreliable with small
samples.

A second approach to determining p-values is a likelihood ratio test for
nested models. Let
\(l(\bm{\hat{\theta}}_0, \bm{\hat{\beta}}_0 | \mathbf{y} )\) be the
log-likelihood from some reduced model and
\(l(\bm{\hat{\theta}}_1, \bm{\hat{\beta}}_1 | \mathbf{y} )\) be the
log-likelihood from some full model. When the reduced model is nested in
the full model (i.e., the reduced model can be obtained by fixing some
parameters of the full model),
\([-2l(\bm{\hat{\theta}}_0, \bm{\hat{\beta}}_0 | \mathbf{y} )] - [-2l(\bm{\hat{\theta}}_1, \bm{\hat{\beta}}_1 | \mathbf{y} )]\)
is asymptotically Chi-squared with degrees of freedom equal to the
difference in estimated parameters between the full and reduced model.

For REML estimation, likelihood ratio tests can only be used to compare
nested models whose fixed effect structure does not change. This is
because the REML likelihood \eqref{eq:reml-lik} depends on the fixed
effects through
\(\ln{|\mathbf{X}^\intercal \mathbf{\Sigma}^{-1} \mathbf{X}|}\). To use
likelihood ratio tests for assessing the importance of fixed effects,
parameters must be estimated using ML. When using likelihood ratio tests
to assess the importance of parameters on the boundary of a parameter
space (e.g., a variance parameter being zero), p-values tend to be too
large (Self and Liang 1987; Stram and Lee 1994; Goldman, Whelan, and
Simon 2000; Pinheiro and Bates 2006).

\hypertarget{coef}{%
\subsection{\texorpdfstring{\texttt{coef()}}{coef()}}\label{coef}}

\texttt{coef()} returns relevant coefficients based on the \texttt{type}
argument. When \texttt{type\ =\ "fixed"} (the default), \texttt{coef()}
returns
\(\hat{\boldsymbol{\beta}} = (\mathbf{X}^\intercal \hat{\boldsymbol{\Sigma}}^{-1} \mathbf{X})^{-1}\mathbf{X}^\intercal \hat{\boldsymbol{\Sigma}}^{-1} \mathbf{y}\),
where \(\hat{\boldsymbol{\Sigma}}^{-1}\) is the inverse covariance
matrix evaluated at the estimated covariance parameters. If the
estimation method is restricted maximum likelihood or maximum
likelihood, this estimator is known as the restricted maximum likelihood
or maximum likelihood estimator of \(\boldsymbol{\beta}\). If the
estimation method is semivariogram weighted least squares or
semivariogram composite likelihood, this estimator is known as the
empirical generalized least squares estimator of \(\boldsymbol{\beta}\).
When \texttt{type\ =\ "spcov"}, the estimated spatial covariance
parameters are returned (available for all estimation methods). When
\texttt{type\ =\ "randcov"}, the estimated random effect variance
parameters are returned (available for restricted maximum likelihood and
maximum likelihood estimation methods).

\hypertarget{confint}{%
\subsection{\texorpdfstring{\texttt{confint()}}{confint()}}\label{confint}}

\texttt{confint()} returns confidence intervals for estimated
parameters. Currently, \texttt{confint()} only returns confidence
intervals for the fixed effects. The confidence interval for \(\beta_i\)
is
\(\beta_i \pm z^* \sqrt{(\mathbf{X}^\intercal \hat{\boldsymbol{\Sigma}}^{-1} \mathbf{X})^{-1}_{ii}}\),
where \(\Phi(z^*) = 1 - \alpha / 2\), \(\Phi(\cdot)\) is the standard
normal (Gaussian) cumulative distribution function, \(\alpha = 1 -\)
\texttt{level}, and
\((\mathbf{X}^\intercal \hat{\boldsymbol{\Sigma}}^{-1} \mathbf{X})^{-1}_{ii}\)
is the \(i\)th diagonal element in
\((\mathbf{X}^\intercal \hat{\boldsymbol{\Sigma}}^{-1} \mathbf{X})^{-1}\).
The default for \texttt{level} is 0.95, which corresponds to a \(z^*\)
of approximately 1.96.

\hypertarget{cooks.distance}{%
\subsection{\texorpdfstring{\texttt{cooks.distance()}}{cooks.distance()}}\label{cooks.distance}}

Cook's distance measures the influence of an observation. For
independent observations, the Cook's distance of a linear model
\(\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}\)
is
\(\hat{\boldsymbol{\epsilon}} (\mathbf{H}_{ii} / [p(1 - \mathbf{H}_{ii})]\),
where \(p\) is the number of fixed effects and \(\mathbf{H}_{ii}\) is
the hat matrix equal to
\(\mathbf{X} (\mathbf{X}^\intercal \mathbf{X})^{-1} \mathbf{X}^\intercal\).
For dependent observations, the Cook's distance can be viewed as the
standard Cook's distance of ``whitened'' observations corresponding to
the linear model
\(\mathbf{y}^* = \mathbf{X}^* \boldsymbol{\beta} + \boldsymbol{\epsilon}^*\),
where \(\mathbf{y}^* = \boldsymbol{\Sigma}^{-1/2} \mathbf{y}\),
\(\mathbf{X}^* = \boldsymbol{\Sigma}^{-1/2} \mathbf{y}\), and
\(\boldsymbol{\epsilon}^* = \boldsymbol{\Sigma}^{-1/2} \boldsymbol{\epsilon}\).
So the whitened Cook's distance is
\((\hat{\boldsymbol{\epsilon}}^*_i (\mathbf{H}^*_{ii} / [p(1 - \mathbf{H}^*_{ii})])\),
where \(\mathbf{H}^*_{ii}\) is the \(i\)th diagonal entry of the hat
matrix
\(\mathbf{H}^* = \mathbf{X}^* [(\mathbf{X}^*)^\intercal \mathbf{X})]^{-1} (\mathbf{X}^*)^\intercal\)
and \(\hat{\boldsymbol{\Sigma}}\) is the covariance matrix evaluated at
the estimated covariance parameters. Suggested cutoffs for identifying
influential observations are \(4 / n\), where \(n\) is the sample size,
or 1.

\hypertarget{deviance}{%
\subsection{\texorpdfstring{\texttt{deviance()}}{deviance()}}\label{deviance}}

The deviance of a fitted model is
\(2\ln(\hat{\mathbf{L}}_s) - 2\ln(\hat{\mathbf{L}}_f)\), where
\(2\ln(\hat{L}_s)\) is twice the log-likelihood of a ``saturated'' model
that fits every observation perfectly and \(2\ln(\hat{L}_f)\) is twice
the log-likelihood of the fitted model. For normal (Gaussian) data, the
deviance of a fitted model is
\((\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})^\intercal \hat{\boldsymbol{\Sigma}}^{-1} (\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})\).

\hypertarget{esv}{%
\subsection{\texorpdfstring{\texttt{esv()}}{esv()}}\label{esv}}

\hypertarget{fitted}{%
\subsection{\texorpdfstring{\texttt{fitted()}}{fitted()}}\label{fitted}}

Fitted values for the response, spatial random errors, and random
effects are obtained in spmodel using \texttt{fitted()}. The fitted
values for the response, denoted \(\mathbf{\hat{y}}\), are given by
\begin{equation}\label{eq:fit_resp}
  \mathbf{\hat{y}} = \mathbf{X} \bm{\hat{\beta}} .
\end{equation} They are the estimated mean response given the set of
covariates for each observation.

Fitted values for spatial random errors and random effects are linked
with best linear unbiased predictors from mixed model theory. Consider
the standard random effects parameterization \begin{equation}
  \mathbf{y} = \mathbf{X} \bm{\beta} + \mathbf{Z} \mathbf{u} + \bm{\epsilon},
\end{equation} where \(\mathbf{Z}\) denotes the random effects design
matrix, \(\mathbf{u}\) denotes the random effects, and \(\bm{\epsilon}\)
denotes independent random error. Henderson (1975) states that the best
linear unbiased predictor (BLUP) of a single random effect
\(\mathbf{u}\), denoted \(\mathbf{\hat{u}}\), is given by
\begin{equation}\label{eq:blup_mm}
  \mathbf{\hat{u}} = \sigma^2_u \mathbf{Z} \mathbf{\Sigma}^{-1}(\mathbf{y} - \mathbf{X} \bm{\hat{\beta}}).
\end{equation} Searle, Casella, and McCulloch (2009) generalizes this
idea by showing that for a random variable \(\bm{\alpha}\), the best
linear unbiased predictor (based on the response, \(\mathbf{y}\)) of
\(\mathbf{\alpha}\), denoted \(\bm{\hat{\alpha}}\) is given by
\begin{equation}\label{eq:blup_gen}
  \bm{\hat{\alpha}} = \text{E}(\bm{\alpha}) + \Sigma_\alpha \Sigma^{-1}(\mathbf{y} - \mathbf{X} \bm{\hat{\beta}}),
\end{equation} where
\(\Sigma_\alpha = \text{Cov}(\bm{\alpha}, \mathbf{y})\). Evaluating
Equation\(~\)\eqref{eq:blup_gen} at the plug-in (empirical) estimates of
the covariance parameters yields the empirical best linear unbiased
predictor (EBLUP) of \(\bm{\alpha}\).

Building from this idea and putting plug-in (empirical) estimates of the
covariance parameters into Equation\(~\)\eqref{eq:blup_gen} yields
predictors for each random error, which we call fitted values
henceforth. For example, the fitted value corresponding to the spatial
dependent random error, denoted \(\bm{\hat{\tau}}\), is given by
\begin{equation}\label{eq:blup_sp}
  \bm{\hat{\tau}} = \mathbf{\Sigma}_{de} \mathbf{\Sigma}^{-1}(\mathbf{y} - \mathbf{X} \bm{\hat{\beta}}),
\end{equation} where
\(\mathbf{\Sigma}_{de} = \text{Cov}(\bm{\tau}, \mathbf{y})\). To obtain
these fitted values for the spatial random errors, run
\texttt{fitted(object,\ type\ =\ "spcov")}. To obtain these fitted
values for the random effects, run
\texttt{fitted(object,\ type\ =\ "randcov")}.

(Talk about assumption of independent among the random effects yields
the EBLUPs for us?)

\hypertarget{hatvalues}{%
\subsection{\texorpdfstring{\texttt{hatvalues()}}{hatvalues()}}\label{hatvalues}}

Hat values measure the leverage of an observation. For independent
observations, the hat matrix of a linear model
\(\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}\)
is
\(\mathbf{H} = \mathbf{X} (\mathbf{X}^\intercal \mathbf{X})^{-1} \mathbf{X}^\intercal\).
The leverage of the \(i\)th observation is the \(i\)th diagonal element
of \(\mathbf{H}\). For dependent observations, the Hat matrix can be
viewed as the standard Hat matrix of ``whitened'' observations
corresponding to the linear model
\(\mathbf{y}^* = \mathbf{X}^* \boldsymbol{\beta} + \boldsymbol{\epsilon}^*\),
where \(\mathbf{y}^* = \boldsymbol{\Sigma}^{-1/2} \mathbf{y}\),
\(\mathbf{X}^* = \boldsymbol{\Sigma}^{-1/2} \mathbf{y}\), and
\(\boldsymbol{\epsilon}^* = \boldsymbol{\Sigma}^{-1/2} \boldsymbol{\epsilon}\).
So the whitened Hat matrix is
\(\mathbf{H}^* = \mathbf{X}^* [(\mathbf{X}^*)^\intercal \mathbf{X}]^{-1} (\mathbf{X}^*)^\intercal\)
and \(\hat{\boldsymbol{\Sigma}}\) is the covariance matrix evaluated at
the estimated covariance parameters. Suggested cutoffs for identifying
observations with high leverage are \(2p / n\) or \(3p / n\), where
\(p\) is the number of fixed effects and \(n\) is the sample size.

\hypertarget{loglik}{%
\subsection{\texorpdfstring{\texttt{logLik()}}{logLik()}}\label{loglik}}

\texttt{logLik()} returns the value of the log-likelihood for fitted
models estimated using restricted maximum likelihood or maximum
likelihood.

\hypertarget{loocv}{%
\subsection{\texorpdfstring{\texttt{loocv()}}{loocv()}}\label{loocv}}

\hypertarget{plot}{%
\subsection{\texorpdfstring{\texttt{plot()}}{plot()}}\label{plot}}

\hypertarget{predict}{%
\subsection{\texorpdfstring{\texttt{predict()}}{predict()}}\label{predict}}

Predictions are often the primary goal of a data analysis. spmodel
performs best linear unbiased prediction, which is equivalent to
Kriging, using the \texttt{predict()} function. Let \(\mathbf{y}_o\) be
a vector of observed variables and \(\mathbf{y}_u\) be a vector of
unobserved variables. Let \(\mathbf{X}_o\) and \(\mathbf{X}_u\) be the
fixed effect design matrices of \(\mathbf{y}_o\) and \(\mathbf{y}_u\),
respectively. Let \(\mathbf{\Sigma}_o\) and \(\mathbf{\Sigma}_u\) be the
covariance matrices of \(\mathbf{y}_o\) and \(\mathbf{y}_u\),
respectively. The (empirical) best linear unbiased predictor (BLUP;
Kriging predictor) of \(\mathbf{y}_u\), denoted \(\mathbf{\dot{y}_u}\)
is given by \begin{equation}\label{eq:blup}
  \mathbf{\dot{y}_u} =  \mathbf{X}_u \bm{\hat{\beta}} + \mathbf{\hat{c}}_{uo} \mathbf{\hat{\Sigma}}^{-1}_o (\mathbf{y}_o - \mathbf{X}_o \bm{\hat{\beta}}) ,
\end{equation} where
\(\mathbf{\hat{c}}_{uo} = \hat{\text{Cov}}(\mathbf{y}_u, \mathbf{y}_o)\)
and
\(\bm{\hat{\beta}} = (\mathbf{X}^\intercal \mathbf{\hat{\Sigma}}^{-1} \mathbf{X})^{-1}\).
One can show that \(\hat{\text{Cov}}(\mathbf{\dot{y}})\) is
\begin{equation}\label{eq:vblup}
  \text{Cov}(\mathbf{\dot{y}}) = \mathbf{\hat{\Sigma}}_u - \mathbf{\hat{c}}_{uo} \mathbf{\hat{\Sigma}}^{-1}_o \mathbf{\hat{c}}_{uo}^\intercal + \mathbf{H}(\mathbf{X}^\intercal \mathbf{\hat{\Sigma}}^{-1} \mathbf{X})^{-1}\mathbf{H}^\intercal ,
\end{equation} where
\(\mathbf{H} = (\mathbf{X}_u - \mathbf{\hat{c}}_{uo} \mathbf{\hat{\Sigma}}^{-1}_o \mathbf{X}_o)\).
When \(\texttt{predict(..., interval = "none")}\), predictions are
obtained using Equation\(~\)\ref{eq:blup}. If \texttt{se.fit = TRUE},
standard errors are the square root of the diagonal of
Equation\(~\)\ref{eq:vblup}. When
\texttt{predict(..., interval = "prediction")}, predictions are obtained
using Equation\(~\)\ref{eq:blup} and standard errors used for the lower
and upper bound of the prediction intervals are obtained via the square
root of the diagonal of Equation\(~\)\ref{eq:vblup} (these are also what
are returned if \texttt{se.fit = TRUE}). When
\texttt{predict(..., interval = "confidence")}, mean estimates are
obtained using \(\mathbf{X}_u \bm{\hat{\beta}}\) and standard errors
used for the lower and upper bound of the confidence intervals are
obtained via the square root of the diagonal of
\((\mathbf{X}^\intercal \mathbf{\hat{\Sigma}}^{-1} \mathbf{X})^{-1}\)
(these are also what are returned if \texttt{se.fit = TRUE}).

For autoregressive models, an extra step is required to obtain
\(\hat{\Sigma}^{-1}_o\) and \(\hat{\Sigma}^{-1}_u\), as they depend on
one another through the locations of \(\mathbf{y}_o\) and
\(\mathbf{y}_u\). Let \(\hat{\Sigma}^{-1}\) be the inverse covariance
matrix of \(\mathbf{y}_o\) and \(\mathbf{y}_u\) (which we directly
specify for autoregressive models). If the data are small enough, then
\(\hat{\Sigma}^{-1}\) can be inverted, yielding \(\hat{\Sigma}\). Then
appropriate subsets can then used to obtain the components needed for
prediction. A faster way to obtain relevant components required for
prediction does exist and is worth mentioning -- \(\hat{\Sigma}^{-1}\)
can be represented blockwise as \begin{equation}\label{eq:auto_hw}
  \hat{\Sigma}^{-1} =
  \begin{bmatrix}
    \mathbf{\hat{\Sigma}}^{*}_{o, o} & \mathbf{\hat{\Sigma}}^{*}_{o, u} \\
    \mathbf{\hat{\Sigma}}^{*}_{u, o} & \mathbf{\hat{\Sigma}}^{*}_{u, u}
  \end{bmatrix},
\end{equation} where the dimensions of the blocks are indexed by the
matrix subscripts (rows then columns). All of the terms required for
prediction can be obtained from this block representation. Wolf (1978)
shows that \begin{equation}\label{eq:hw_forms}
  \begin{split}
    \hat{\Sigma}^{-1}_o & = \mathbf{\hat{\Sigma}}^{*}_{o, o} - \mathbf{\hat{\Sigma}}^{*}_{o, u} (\mathbf{\hat{\Sigma}}^{*}_{u, u})^{-1} \mathbf{\hat{\Sigma}}^{*}_{u, o} \\
    \hat{\Sigma}_u & = (\mathbf{\hat{\Sigma}}^{*}_{u, u} - \mathbf{\hat{\Sigma}}^{*}_{u, o} (\mathbf{\hat{\Sigma}}^{*}_{o, o})^{-1} \mathbf{\hat{\Sigma}}^{*}_{o, u})^{-1} \\
    \text{Cov}(\mathbf{y}_o, \mathbf{y}_u) & = - \hat{\Sigma}_o \mathbf{\hat{\Sigma}}^{*}_{o, u} (\mathbf{\hat{\Sigma}}^{*}_{u, u})^{-1}
  \end{split}
\end{equation} A similar result exists for the log determinant (used in
ML and REML estimation).

\hypertarget{pseudor2}{%
\subsection{\texorpdfstring{\texttt{pseudoR2()}}{pseudoR2()}}\label{pseudor2}}

\texttt{pseudoR2()} is the pseudo r-squared equaling
\(1 - \mathbf{D}_f / \mathbf{D}_n\), where \(\mathbf{D}_f\) is the
deviance of the fitted model and \(\mathbf{D}_n\) is the deviance of an
intercept-only model (with the same estimated covariance parameters as
for the fitted model). If the observations are independent and normal
(Gaussian), this pseudo r-squared equals the classical r-squared
\(1 - SSE / SST\) for
\(SSE = (\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})^\intercal (\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})\)
and
\(SST = (\mathbf{y} - \hat{\mu})^\intercal (\mathbf{y} - \mathbf{X} \hat{\mu})\)
for intercept \(\hat{\mu}\).

\hypertarget{residuals}{%
\subsection{\texorpdfstring{\texttt{residuals()}}{residuals()}}\label{residuals}}

\texttt{residuals()} returns various fitted-model residuals. When
\texttt{type\ =\ "raw"}, the raw residuals are returned,
\(\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}}\). When
\texttt{type\ =\ "normalized"}, the normalized residuals are returned,
\(\hat{\boldsymbol{\Sigma}}^{-1/2}(\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})\).
When \texttt{type\ =\ "standardized"}, the standardized residuals are
returned,
\(\hat{\boldsymbol{\Sigma}}^{-1/2}(\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})/ \sqrt{(1 - diag(\mathbf{H}^*))}\),
where \(diag(\mathbf{H}^*)\) is the diagonal of the ``whitened'' hat
matrix, \(\mathbf{H}^*\).

\hypertarget{sprnorm}{%
\subsection{\texorpdfstring{\texttt{sprnorm()}}{sprnorm()}}\label{sprnorm}}

\hypertarget{vcov}{%
\subsection{\texorpdfstring{\texttt{vcov()}}{vcov()}}\label{vcov}}

\texttt{vcov()} returns the variance-covariance matrix of estimated
parameters. Currently, \texttt{vcov()} only returns the
variance-covariance matrix of the fixed effects. The variance-covariance
matrix of the fixed effects is given by
\((\mathbf{X}^\intercal \hat{\boldsymbol{\Sigma}}^{-1} \mathbf{X})^{-1}\).

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-cressie1985fitting}{}%
Cressie, Noel. 1985. ``Fitting Variogram Models by Weighted Least
Squares.'' \emph{Journal of the International Association for
Mathematical Geology} 17 (5): 563--86.

\leavevmode\hypertarget{ref-cressie1993statistics}{}%
---------. 1993. \emph{Statistics for Spatial Data}. John Wiley \& Sons.

\leavevmode\hypertarget{ref-curriero1999composite}{}%
Curriero, Frank C, and Subhash Lele. 1999. ``A Composite Likelihood
Approach to Semivariogram Estimation.'' \emph{Journal of Agricultural,
Biological, and Environmental Statistics}, 9--28.

\leavevmode\hypertarget{ref-goldman2000statistical}{}%
Goldman, Nick, Whelan, and Simon. 2000. ``Statistical Tests of
Gamma-Distributed Rate Heterogeneity in Models of Sequence Evolution in
Phylogenetics.'' \emph{Molecular Biology and Evolution} 17 (6): 975--78.

\leavevmode\hypertarget{ref-harville1977maximum}{}%
Harville, David A. 1977. ``Maximum Likelihood Approaches to Variance
Component Estimation and to Related Problems.'' \emph{Journal of the
American Statistical Association} 72 (358): 320--38.

\leavevmode\hypertarget{ref-harville1992mean}{}%
Harville, David A, and Daniel R Jeske. 1992. ``Mean Squared Error of
Estimation or Prediction Under a General Linear Model.'' \emph{Journal
of the American Statistical Association} 87 (419): 724--31.

\leavevmode\hypertarget{ref-henderson1975best}{}%
Henderson, Charles R. 1975. ``Best Linear Unbiased Estimation and
Prediction Under a Selection Model.'' \emph{Biometrics}, 423--47.

\leavevmode\hypertarget{ref-hoeting2006model}{}%
Hoeting, Jennifer A, Richard A Davis, Andrew A Merton, and Sandra E
Thompson. 2006. ``Model Selection for Geostatistical Models.''
\emph{Ecological Applications} 16 (1): 87--98.

\leavevmode\hypertarget{ref-hrong1996approximate}{}%
Hrong-Tai Fai, Alex, and Paul L Cornelius. 1996. ``Approximate F-Tests
of Multiple Degree of Freedom Hypotheses in Generalized Least Squares
Analyses of Unbalanced Split-Plot Experiments.'' \emph{Journal of
Statistical Computation and Simulation} 54 (4): 363--78.

\leavevmode\hypertarget{ref-kackar1984approximations}{}%
Kackar, Raghu N, and David A Harville. 1984. ``Approximations for
Standard Errors of Estimators of Fixed and Random Effects in Mixed
Linear Models.'' \emph{Journal of the American Statistical Association}
79 (388): 853--62.

\leavevmode\hypertarget{ref-kenward1997small}{}%
Kenward, Michael G, and James H Roger. 1997. ``Small Sample Inference
for Fixed Effects from Restricted Maximum Likelihood.''
\emph{Biometrics}, 983--97.

\leavevmode\hypertarget{ref-kenward2009improved}{}%
---------. 2009. ``An Improved Approximation to the Precision of Fixed
Effects from Restricted Maximum Likelihood.'' \emph{Computational
Statistics \& Data Analysis} 53 (7): 2583--95.

\leavevmode\hypertarget{ref-littell2006sas}{}%
Littell, Ramon C, George A Milliken, Walter W Stroup, Russell D
Wolfinger, and Schabenberber Oliver. 2006. \emph{SAS for Mixed Models}.
SAS publishing.

\leavevmode\hypertarget{ref-patterson1971recovery}{}%
Patterson, H Desmond, and Robin Thompson. 1971. ``Recovery of
Inter-Block Information When Block Sizes Are Unequal.''
\emph{Biometrika} 58 (3): 545--54.

\leavevmode\hypertarget{ref-pinheiro2006mixed}{}%
Pinheiro, José, and Douglas Bates. 2006. \emph{Mixed-Effects Models in S
and S-Plus}. Springer science \& business media.

\leavevmode\hypertarget{ref-prasad1990estimation}{}%
Prasad, NG Narasimha, and Jon NK Rao. 1990. ``The Estimation of the Mean
Squared Error of Small-Area Estimators.'' \emph{Journal of the American
Statistical Association} 85 (409): 163--71.

\leavevmode\hypertarget{ref-satterthwaite1946approximate}{}%
Satterthwaite, Franklin E. 1946. ``An Approximate Distribution of
Estimates of Variance Components.'' \emph{Biometrics Bulletin} 2 (6):
110--14.

\leavevmode\hypertarget{ref-schluchter1990small}{}%
Schluchter, Mark D, and Janet T Elashoff. 1990. ``Small-Sample
Adjustments to Tests with Unbalanced Repeated Measures Assuming Several
Covariance Structures.'' \emph{Journal of Statistical Computation and
Simulation} 37 (1-2): 69--87.

\leavevmode\hypertarget{ref-searle2009variance}{}%
Searle, Shayle R, George Casella, and Charles E McCulloch. 2009.
\emph{Variance Components}. John Wiley \& Sons.

\leavevmode\hypertarget{ref-self1987asymptotic}{}%
Self, Steven G, and Kung-Yee Liang. 1987. ``Asymptotic Properties of
Maximum Likelihood Estimators and Likelihood Ratio Tests Under
Nonstandard Conditions.'' \emph{Journal of the American Statistical
Association} 82 (398): 605--10.

\leavevmode\hypertarget{ref-stram1994variance}{}%
Stram, Daniel O, and Jae Won Lee. 1994. ``Variance Components Testing in
the Longitudinal Mixed Effects Model.'' \emph{Biometrics}, 1171--7.

\leavevmode\hypertarget{ref-wolf1978helmert}{}%
Wolf, Helmut. 1978. ``The Helmert Block Method-Its Origin and
Development.'' In \emph{Proceedings of the Second International
Symposium on Problems Related to the Redefinition of North American
Geodetic Networks,(NOAA, Arlington-Va, 1978)}, 319--26.

\leavevmode\hypertarget{ref-wolfinger1994computing}{}%
Wolfinger, Russ, Randy Tobias, and John Sall. 1994. ``Computing Gaussian
Likelihoods and Their Derivatives for General Linear Mixed Models.''
\emph{SIAM Journal on Scientific Computing} 15 (6): 1294--1310.

\bibliographystyle{unsrt}
\bibliography{references.bib}


\end{document}
